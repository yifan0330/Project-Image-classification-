{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model,Sequential, load_model\n",
    "from keras.layers import Dense,Activation,Dropout, Flatten\n",
    "from keras.layers import Input,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.xception import Xception\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.losses import softmax_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model with 6 conv layers, BN and lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/40\n",
      "  512/45000 [..............................] - ETA: 17s - loss: 4.0046 - acc: 0.1602"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/incubator-mxnet/python/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.015625). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 10s 220us/step - loss: 1.9427 - acc: 0.3913 - val_loss: 1.3631 - val_acc: 0.5184\n",
      "Epoch 2/40\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 1.2054 - acc: 0.5833 - val_loss: 0.9767 - val_acc: 0.6578\n",
      "Epoch 3/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.9362 - acc: 0.6800 - val_loss: 0.8129 - val_acc: 0.7172\n",
      "Epoch 4/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.7810 - acc: 0.7326 - val_loss: 0.7219 - val_acc: 0.7590\n",
      "Epoch 5/40\n",
      "45000/45000 [==============================] - 36s 808us/step - loss: 0.6866 - acc: 0.7673 - val_loss: 0.6459 - val_acc: 0.7788\n",
      "Epoch 6/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.6135 - acc: 0.7912 - val_loss: 0.6479 - val_acc: 0.7844\n",
      "Epoch 7/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.5566 - acc: 0.8126 - val_loss: 0.5673 - val_acc: 0.8132\n",
      "Epoch 8/40\n",
      "45000/45000 [==============================] - 39s 861us/step - loss: 0.5112 - acc: 0.8289 - val_loss: 0.5558 - val_acc: 0.8142\n",
      "Epoch 9/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.4725 - acc: 0.8419 - val_loss: 0.5629 - val_acc: 0.8168\n",
      "Epoch 10/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.4308 - acc: 0.8531 - val_loss: 0.5282 - val_acc: 0.8262\n",
      "Epoch 11/40\n",
      "45000/45000 [==============================] - 38s 849us/step - loss: 0.4024 - acc: 0.8645 - val_loss: 0.5320 - val_acc: 0.8276\n",
      "Epoch 12/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.3678 - acc: 0.8763 - val_loss: 0.4873 - val_acc: 0.8414\n",
      "Epoch 13/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.3451 - acc: 0.8836 - val_loss: 0.4805 - val_acc: 0.8422\n",
      "Epoch 14/40\n",
      "45000/45000 [==============================] - 38s 847us/step - loss: 0.3284 - acc: 0.8917 - val_loss: 0.4834 - val_acc: 0.8480\n",
      "Epoch 15/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.3006 - acc: 0.8993 - val_loss: 0.5737 - val_acc: 0.8340\n",
      "Epoch 16/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2767 - acc: 0.9062 - val_loss: 0.5183 - val_acc: 0.8440\n",
      "Epoch 17/40\n",
      "45000/45000 [==============================] - 38s 848us/step - loss: 0.2666 - acc: 0.9119 - val_loss: 0.4990 - val_acc: 0.8416\n",
      "Epoch 18/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2473 - acc: 0.9172 - val_loss: 0.4946 - val_acc: 0.8516\n",
      "Epoch 19/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.2420 - acc: 0.9184 - val_loss: 0.5250 - val_acc: 0.8376\n",
      "Epoch 20/40\n",
      "45000/45000 [==============================] - 38s 846us/step - loss: 0.2220 - acc: 0.9265 - val_loss: 0.5027 - val_acc: 0.8442\n",
      "Epoch 21/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.2108 - acc: 0.9303 - val_loss: 0.5707 - val_acc: 0.8482\n",
      "Epoch 22/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.2074 - acc: 0.9319 - val_loss: 0.5955 - val_acc: 0.8396\n",
      "Epoch 23/40\n",
      "45000/45000 [==============================] - 39s 859us/step - loss: 0.1924 - acc: 0.9360 - val_loss: 0.5432 - val_acc: 0.8396\n",
      "Epoch 24/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.1873 - acc: 0.9393 - val_loss: 0.5341 - val_acc: 0.8528\n",
      "Epoch 25/40\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.1786 - acc: 0.9421 - val_loss: 0.7661 - val_acc: 0.8222\n",
      "Epoch 26/40\n",
      "45000/45000 [==============================] - 39s 869us/step - loss: 0.1686 - acc: 0.9444 - val_loss: 0.4719 - val_acc: 0.8632\n",
      "Epoch 27/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.1573 - acc: 0.9473 - val_loss: 0.5278 - val_acc: 0.8618\n",
      "Epoch 28/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.1698 - acc: 0.9464 - val_loss: 0.5430 - val_acc: 0.8542\n",
      "Epoch 29/40\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.1594 - acc: 0.9480 - val_loss: 0.5253 - val_acc: 0.8588\n",
      "Epoch 30/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.1514 - acc: 0.9498 - val_loss: 0.5769 - val_acc: 0.8572\n",
      "Epoch 31/40\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.1439 - acc: 0.9532 - val_loss: 0.5510 - val_acc: 0.8596\n",
      "Epoch 32/40\n",
      "45000/45000 [==============================] - 35s 779us/step - loss: 0.1357 - acc: 0.9553 - val_loss: 0.5712 - val_acc: 0.8602\n",
      "Epoch 33/40\n",
      "45000/45000 [==============================] - 29s 646us/step - loss: 0.1356 - acc: 0.9563 - val_loss: 0.5535 - val_acc: 0.8642\n",
      "Epoch 34/40\n",
      "45000/45000 [==============================] - 29s 650us/step - loss: 0.1342 - acc: 0.9561 - val_loss: 0.5634 - val_acc: 0.8588\n",
      "Epoch 35/40\n",
      "45000/45000 [==============================] - 23s 504us/step - loss: 0.1293 - acc: 0.9590 - val_loss: 0.5099 - val_acc: 0.8748\n",
      "Epoch 36/40\n",
      "45000/45000 [==============================] - 10s 215us/step - loss: 0.1246 - acc: 0.9600 - val_loss: 0.5566 - val_acc: 0.8628\n",
      "Epoch 37/40\n",
      "45000/45000 [==============================] - 10s 213us/step - loss: 0.1198 - acc: 0.9620 - val_loss: 0.6844 - val_acc: 0.8510\n",
      "Epoch 38/40\n",
      "45000/45000 [==============================] - 10s 212us/step - loss: 0.1183 - acc: 0.9624 - val_loss: 0.6694 - val_acc: 0.8526\n",
      "Epoch 39/40\n",
      "45000/45000 [==============================] - 10s 215us/step - loss: 0.1160 - acc: 0.9628 - val_loss: 0.5994 - val_acc: 0.8606\n",
      "Epoch 40/40\n",
      "45000/45000 [==============================] - 10s 213us/step - loss: 0.1172 - acc: 0.9630 - val_loss: 0.5354 - val_acc: 0.8632\n",
      "The test loss is 0.5584078034758568\n",
      "The test accuracy is 0.8592\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 10\n",
    "# x_train: (50000, 32, 32, 3);  y_train: (50000, 1).\n",
    "# x_test: (10000, 32, 32, 3); y_test: (10000, 1).\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Convert classification labels to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, number_of_class)\n",
    "y_test = keras.utils.to_categorical(y_test, number_of_class)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# kernel_size:(3,3)->specify the height and width of the 2D convolution window\n",
    "#\"SAME\": output size is the same as input size. \n",
    "#This requires the filter window to slip outside input map(need to pad)\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#Flatten the input. Do not affect the batch size.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# decay: Learning rate decay over each update\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit_model=model.fit(x_train, y_train,\n",
    "                    batch_size=64,epochs=40,verbose=1,\n",
    "                    validation_split=0.10)\n",
    "model.save('cifar-10')\n",
    "performance = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model with 6 conv layers, BN and lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "  576/45000 [..............................] - ETA: 16s - loss: 4.4437 - acc: 0.1441"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/incubator-mxnet/python/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.015625). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 10s 223us/step - loss: 1.8009 - acc: 0.4068 - val_loss: 2.2645 - val_acc: 0.3988\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 10s 227us/step - loss: 1.1477 - acc: 0.6063 - val_loss: 1.4971 - val_acc: 0.5450\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 10s 232us/step - loss: 0.9738 - acc: 0.6764 - val_loss: 1.4589 - val_acc: 0.4896\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 10s 230us/step - loss: 0.8752 - acc: 0.7147 - val_loss: 1.1123 - val_acc: 0.6424\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 10s 229us/step - loss: 0.8127 - acc: 0.7385 - val_loss: 1.1001 - val_acc: 0.6472\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 10s 229us/step - loss: 0.7656 - acc: 0.7578 - val_loss: 1.3423 - val_acc: 0.6882\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 10s 228us/step - loss: 0.7347 - acc: 0.7717 - val_loss: 1.5000 - val_acc: 0.6136\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 10s 227us/step - loss: 0.6952 - acc: 0.7872 - val_loss: 1.0363 - val_acc: 0.6884\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 10s 225us/step - loss: 0.6797 - acc: 0.7972 - val_loss: 0.6998 - val_acc: 0.7868\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 10s 227us/step - loss: 0.6461 - acc: 0.8066 - val_loss: 1.1329 - val_acc: 0.7324\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 10s 232us/step - loss: 0.6456 - acc: 0.8131 - val_loss: 1.2092 - val_acc: 0.7594\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 10s 223us/step - loss: 0.6260 - acc: 0.8216 - val_loss: 0.8651 - val_acc: 0.7728\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 10s 216us/step - loss: 0.5950 - acc: 0.8293 - val_loss: 0.7546 - val_acc: 0.7784\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 10s 216us/step - loss: 0.5805 - acc: 0.8339 - val_loss: 0.8705 - val_acc: 0.7654\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 10s 215us/step - loss: 0.5661 - acc: 0.8410 - val_loss: 0.7934 - val_acc: 0.7980\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 10s 216us/step - loss: 0.5614 - acc: 0.8463 - val_loss: 0.9983 - val_acc: 0.7212\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 10s 219us/step - loss: 0.5417 - acc: 0.8502 - val_loss: 0.9424 - val_acc: 0.7530\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 10s 215us/step - loss: 0.5379 - acc: 0.8563 - val_loss: 0.8310 - val_acc: 0.8136\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 10s 220us/step - loss: 0.5239 - acc: 0.8627 - val_loss: 0.7945 - val_acc: 0.8086\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 10s 229us/step - loss: 0.5640 - acc: 0.8582 - val_loss: 0.8349 - val_acc: 0.8234\n",
      "The test loss is 0.8864854634523391\n",
      "The test accuracy is 0.8044\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 10\n",
    "# x_train: (50000, 32, 32, 3);  y_train: (50000, 1).\n",
    "# x_test: (10000, 32, 32, 3); y_test: (10000, 1).\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Convert classification labels to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, number_of_class)\n",
    "y_test = keras.utils.to_categorical(y_test, number_of_class)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# kernel_size:(3,3)->specify the height and width of the 2D convolution window\n",
    "#\"SAME\": output size is the same as input size. \n",
    "#This requires the filter window to slip outside input map(need to pad)\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Flatten the input. Do not affect the batch size.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# decay: Learning rate decay over each update\n",
    "opt = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit_model=model.fit(x_train, y_train,\n",
    "                    batch_size=64,epochs=20,verbose=1,\n",
    "                    validation_split=0.10)\n",
    "model.save('cifar-10')\n",
    "performance = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhpmZFXHBMDOzIn8A6QJ/DCfVOE0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0dea2ab8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = fit_model.history['loss']\n",
    "val_loss = fit_model.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'yo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'indigo', label='Validation loss')\n",
    "plt.title('Training loss and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0de9d8fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = fit_model.history['acc']\n",
    "val_acc = fit_model.history['val_acc']\n",
    "plt.plot(epochs, acc, 'yo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'indigo', label='Validation accuracy')\n",
    "plt.title('Training accuracy and validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/40\n",
      "45000/45000 [==============================] - 98s 2ms/step - loss: 1.9872 - acc: 0.2530 - val_loss: 1.9594 - val_acc: 0.3036\n",
      "Epoch 2/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.6617 - acc: 0.3866 - val_loss: 1.5403 - val_acc: 0.4302\n",
      "Epoch 3/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.4912 - acc: 0.4536 - val_loss: 1.5711 - val_acc: 0.4346\n",
      "Epoch 4/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.3729 - acc: 0.5040 - val_loss: 1.5318 - val_acc: 0.4762\n",
      "Epoch 5/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.2632 - acc: 0.5466 - val_loss: 1.1971 - val_acc: 0.5666\n",
      "Epoch 6/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.1806 - acc: 0.5782 - val_loss: 1.1650 - val_acc: 0.5782\n",
      "Epoch 7/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.1144 - acc: 0.6061 - val_loss: 1.1372 - val_acc: 0.5888\n",
      "Epoch 8/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.0583 - acc: 0.6246 - val_loss: 1.2281 - val_acc: 0.5956\n",
      "Epoch 9/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 1.0108 - acc: 0.6433 - val_loss: 1.0384 - val_acc: 0.6370\n",
      "Epoch 10/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.9687 - acc: 0.6590 - val_loss: 1.1901 - val_acc: 0.5962\n",
      "Epoch 11/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.9329 - acc: 0.6692 - val_loss: 1.0773 - val_acc: 0.6344\n",
      "Epoch 12/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.8988 - acc: 0.6841 - val_loss: 0.9419 - val_acc: 0.6650\n",
      "Epoch 13/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.8662 - acc: 0.6966 - val_loss: 0.8276 - val_acc: 0.7132\n",
      "Epoch 14/40\n",
      "45000/45000 [==============================] - 96s 2ms/step - loss: 0.8354 - acc: 0.7091 - val_loss: 0.8100 - val_acc: 0.7162\n",
      "Epoch 15/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.8044 - acc: 0.7203 - val_loss: 0.8462 - val_acc: 0.7156\n",
      "Epoch 16/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.7763 - acc: 0.7308 - val_loss: 0.9660 - val_acc: 0.6822\n",
      "Epoch 17/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.7513 - acc: 0.7376 - val_loss: 0.7584 - val_acc: 0.7392\n",
      "Epoch 18/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.7257 - acc: 0.7468 - val_loss: 0.8897 - val_acc: 0.7040\n",
      "Epoch 19/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.7105 - acc: 0.7539 - val_loss: 0.8345 - val_acc: 0.7362\n",
      "Epoch 20/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6885 - acc: 0.7624 - val_loss: 0.9272 - val_acc: 0.6792\n",
      "Epoch 21/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6729 - acc: 0.7678 - val_loss: 0.7093 - val_acc: 0.7582\n",
      "Epoch 22/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6537 - acc: 0.7750 - val_loss: 0.7809 - val_acc: 0.7458\n",
      "Epoch 23/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6436 - acc: 0.7788 - val_loss: 0.9053 - val_acc: 0.7232\n",
      "Epoch 24/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6347 - acc: 0.7829 - val_loss: 0.6915 - val_acc: 0.7740\n",
      "Epoch 25/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6162 - acc: 0.7890 - val_loss: 0.7058 - val_acc: 0.7646\n",
      "Epoch 26/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.6075 - acc: 0.7920 - val_loss: 0.7326 - val_acc: 0.7430\n",
      "Epoch 27/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5936 - acc: 0.7971 - val_loss: 0.7590 - val_acc: 0.7640\n",
      "Epoch 28/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5846 - acc: 0.8014 - val_loss: 0.7829 - val_acc: 0.7398\n",
      "Epoch 29/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5820 - acc: 0.8014 - val_loss: 0.6643 - val_acc: 0.7732\n",
      "Epoch 30/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5647 - acc: 0.8090 - val_loss: 1.0222 - val_acc: 0.7082\n",
      "Epoch 31/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5609 - acc: 0.8107 - val_loss: 0.7409 - val_acc: 0.7614\n",
      "Epoch 32/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5535 - acc: 0.8114 - val_loss: 0.6184 - val_acc: 0.7926\n",
      "Epoch 33/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5423 - acc: 0.8162 - val_loss: 0.5761 - val_acc: 0.8036\n",
      "Epoch 34/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5433 - acc: 0.8153 - val_loss: 0.6780 - val_acc: 0.7736\n",
      "Epoch 35/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5292 - acc: 0.8220 - val_loss: 0.5840 - val_acc: 0.7994\n",
      "Epoch 36/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5194 - acc: 0.8248 - val_loss: 0.6000 - val_acc: 0.8036\n",
      "Epoch 37/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5183 - acc: 0.8234 - val_loss: 0.6966 - val_acc: 0.7774\n",
      "Epoch 38/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5143 - acc: 0.8282 - val_loss: 0.6157 - val_acc: 0.8018\n",
      "Epoch 39/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5069 - acc: 0.8280 - val_loss: 0.6021 - val_acc: 0.8000\n",
      "Epoch 40/40\n",
      "45000/45000 [==============================] - 97s 2ms/step - loss: 0.5045 - acc: 0.8292 - val_loss: 1.0304 - val_acc: 0.7220\n",
      "The test loss is 1.0789127007484436\n",
      "The test accuracy is 0.7081\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 10\n",
    "# x_train: (50000, 32, 32, 3);  y_train: (50000, 1).\n",
    "# x_test: (10000, 32, 32, 3); y_test: (10000, 1).\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Convert classification labels to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, number_of_class)\n",
    "y_test = keras.utils.to_categorical(y_test, number_of_class)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# kernel_size:(3,3)->specify the height and width of the 2D convolution window\n",
    "#\"SAME\": output size is the same as input size. \n",
    "#This requires the filter window to slip outside input map(need to pad)\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#Flatten the input. Do not affect the batch size.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# decay: Learning rate decay over each update\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit_model=model.fit(x_train, y_train,\n",
    "                    batch_size=64,epochs=40,verbose=1,\n",
    "                    validation_split=0.10)\n",
    "model.save('cifar-10')\n",
    "performance = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable weights before frozen: 216\n",
      "The number of trainable weights after frozen: 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 127s 3ms/step - loss: 2.7616 - acc: 0.2037 - val_loss: 2.9183 - val_acc: 0.1240\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 2.1380 - acc: 0.2920 - val_loss: 3.0783 - val_acc: 0.1102\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.9519 - acc: 0.3300 - val_loss: 3.2233 - val_acc: 0.1090\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 124s 3ms/step - loss: 1.8477 - acc: 0.3561 - val_loss: 3.3115 - val_acc: 0.0978\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.7989 - acc: 0.3698 - val_loss: 3.4414 - val_acc: 0.0964\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.7510 - acc: 0.3856 - val_loss: 3.6260 - val_acc: 0.0958\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 124s 3ms/step - loss: 1.7229 - acc: 0.3899 - val_loss: 3.7255 - val_acc: 0.0960\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6970 - acc: 0.4021 - val_loss: 3.7651 - val_acc: 0.0962\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6790 - acc: 0.4076 - val_loss: 3.8976 - val_acc: 0.0960\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6629 - acc: 0.4129 - val_loss: 4.0177 - val_acc: 0.0964\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6506 - acc: 0.4178 - val_loss: 4.0847 - val_acc: 0.0964\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 124s 3ms/step - loss: 1.6275 - acc: 0.4241 - val_loss: 4.1163 - val_acc: 0.0958\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6218 - acc: 0.4282 - val_loss: 4.2129 - val_acc: 0.0958\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.6076 - acc: 0.4324 - val_loss: 4.2985 - val_acc: 0.0958\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5961 - acc: 0.4386 - val_loss: 4.3795 - val_acc: 0.0958\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5870 - acc: 0.4411 - val_loss: 4.4406 - val_acc: 0.0958\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 124s 3ms/step - loss: 1.5766 - acc: 0.4461 - val_loss: 4.5333 - val_acc: 0.0958\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5711 - acc: 0.4473 - val_loss: 4.5816 - val_acc: 0.0958\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 124s 3ms/step - loss: 1.5674 - acc: 0.4460 - val_loss: 4.6700 - val_acc: 0.0958\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5535 - acc: 0.4545 - val_loss: 4.7231 - val_acc: 0.0958\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5532 - acc: 0.4546 - val_loss: 4.8101 - val_acc: 0.0958\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5522 - acc: 0.4530 - val_loss: 4.8235 - val_acc: 0.0958\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5436 - acc: 0.4548 - val_loss: 4.8749 - val_acc: 0.0958\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5360 - acc: 0.4602 - val_loss: 4.9082 - val_acc: 0.0958\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5268 - acc: 0.4606 - val_loss: 4.9354 - val_acc: 0.0960\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5267 - acc: 0.4631 - val_loss: 5.0210 - val_acc: 0.0974\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5177 - acc: 0.4662 - val_loss: 5.0130 - val_acc: 0.0972\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5088 - acc: 0.4661 - val_loss: 5.0526 - val_acc: 0.0970\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 123s 3ms/step - loss: 1.5120 - acc: 0.4653 - val_loss: 5.0865 - val_acc: 0.0960\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5119 - acc: 0.4694 - val_loss: 5.1497 - val_acc: 0.0960\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.5001 - acc: 0.4737 - val_loss: 5.1717 - val_acc: 0.0962\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4992 - acc: 0.4726 - val_loss: 5.2803 - val_acc: 0.0958\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4966 - acc: 0.4737 - val_loss: 5.3015 - val_acc: 0.0958\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4935 - acc: 0.4738 - val_loss: 5.3148 - val_acc: 0.0958\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4895 - acc: 0.4748 - val_loss: 5.3810 - val_acc: 0.0958\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4810 - acc: 0.4776 - val_loss: 5.4745 - val_acc: 0.0958\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4750 - acc: 0.4816 - val_loss: 5.4610 - val_acc: 0.0958\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4748 - acc: 0.4839 - val_loss: 5.5053 - val_acc: 0.0960\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4778 - acc: 0.4797 - val_loss: 5.5270 - val_acc: 0.0960\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4757 - acc: 0.4821 - val_loss: 5.5350 - val_acc: 0.0964\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4620 - acc: 0.4849 - val_loss: 5.6409 - val_acc: 0.0958\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4668 - acc: 0.4849 - val_loss: 5.5979 - val_acc: 0.0964\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4652 - acc: 0.4830 - val_loss: 5.6697 - val_acc: 0.0962\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4653 - acc: 0.4847 - val_loss: 5.6858 - val_acc: 0.0970\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4575 - acc: 0.4896 - val_loss: 5.6660 - val_acc: 0.0962\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4525 - acc: 0.4908 - val_loss: 5.6932 - val_acc: 0.0962\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4460 - acc: 0.4910 - val_loss: 5.6962 - val_acc: 0.0968\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4465 - acc: 0.4892 - val_loss: 5.7535 - val_acc: 0.0970\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4465 - acc: 0.4922 - val_loss: 5.7920 - val_acc: 0.0968\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.4439 - acc: 0.4950 - val_loss: 5.8008 - val_acc: 0.0972\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4405 - acc: 0.4921 - val_loss: 5.8699 - val_acc: 0.1000\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4345 - acc: 0.4952 - val_loss: 5.8848 - val_acc: 0.1000\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4337 - acc: 0.4952 - val_loss: 5.8998 - val_acc: 0.0972\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4298 - acc: 0.4951 - val_loss: 5.9763 - val_acc: 0.0968\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4308 - acc: 0.4962 - val_loss: 5.9820 - val_acc: 0.0990\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4301 - acc: 0.4965 - val_loss: 5.8972 - val_acc: 0.1000\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4283 - acc: 0.4972 - val_loss: 5.9779 - val_acc: 0.0966\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4261 - acc: 0.4972 - val_loss: 5.9473 - val_acc: 0.0964\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4262 - acc: 0.4999 - val_loss: 5.9526 - val_acc: 0.1000\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4289 - acc: 0.4966 - val_loss: 6.0040 - val_acc: 0.0970\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4210 - acc: 0.4995 - val_loss: 5.9846 - val_acc: 0.0964\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4153 - acc: 0.5019 - val_loss: 6.0172 - val_acc: 0.0966\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4162 - acc: 0.5033 - val_loss: 6.0359 - val_acc: 0.0968\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.4124 - acc: 0.5062 - val_loss: 6.0965 - val_acc: 0.0962\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4167 - acc: 0.4994 - val_loss: 6.0924 - val_acc: 0.0976\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4076 - acc: 0.5032 - val_loss: 6.1197 - val_acc: 0.1090\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4059 - acc: 0.5034 - val_loss: 6.1161 - val_acc: 0.1078\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4037 - acc: 0.5062 - val_loss: 6.1278 - val_acc: 0.0980\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4023 - acc: 0.5039 - val_loss: 6.2092 - val_acc: 0.1002\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.4049 - acc: 0.5043 - val_loss: 6.2352 - val_acc: 0.1112\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3993 - acc: 0.5054 - val_loss: 6.2739 - val_acc: 0.0970\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.4016 - acc: 0.5077 - val_loss: 6.2570 - val_acc: 0.1084\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3951 - acc: 0.5083 - val_loss: 6.2596 - val_acc: 0.0982\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.3922 - acc: 0.5124 - val_loss: 6.2985 - val_acc: 0.0974\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3934 - acc: 0.5099 - val_loss: 6.3428 - val_acc: 0.0996\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3946 - acc: 0.5090 - val_loss: 6.3329 - val_acc: 0.1006\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3978 - acc: 0.5090 - val_loss: 6.2862 - val_acc: 0.0966\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3842 - acc: 0.5124 - val_loss: 6.3160 - val_acc: 0.1236\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3839 - acc: 0.5112 - val_loss: 6.2953 - val_acc: 0.0980\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3857 - acc: 0.5138 - val_loss: 6.3517 - val_acc: 0.1092\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3815 - acc: 0.5134 - val_loss: 6.4134 - val_acc: 0.1068\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3856 - acc: 0.5155 - val_loss: 6.3590 - val_acc: 0.1008\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 122s 3ms/step - loss: 1.3799 - acc: 0.5129 - val_loss: 6.4143 - val_acc: 0.1226\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3780 - acc: 0.5141 - val_loss: 6.4623 - val_acc: 0.1238\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3767 - acc: 0.5163 - val_loss: 6.4295 - val_acc: 0.1056\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3793 - acc: 0.5136 - val_loss: 6.4852 - val_acc: 0.1228\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3757 - acc: 0.5150 - val_loss: 6.4857 - val_acc: 0.1018\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3723 - acc: 0.5152 - val_loss: 6.4959 - val_acc: 0.1164\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3692 - acc: 0.5168 - val_loss: 6.4552 - val_acc: 0.1218\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3698 - acc: 0.5183 - val_loss: 6.4704 - val_acc: 0.1008\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3691 - acc: 0.5189 - val_loss: 6.5456 - val_acc: 0.0994\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3680 - acc: 0.5205 - val_loss: 6.5939 - val_acc: 0.1070\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3692 - acc: 0.5174 - val_loss: 6.5742 - val_acc: 0.1220\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3593 - acc: 0.5198 - val_loss: 6.5834 - val_acc: 0.1088\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3589 - acc: 0.5229 - val_loss: 6.6867 - val_acc: 0.1226\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3598 - acc: 0.5226 - val_loss: 6.7267 - val_acc: 0.0990\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3570 - acc: 0.5196 - val_loss: 6.7685 - val_acc: 0.1076\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3591 - acc: 0.5219 - val_loss: 6.7500 - val_acc: 0.1066\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3545 - acc: 0.5220 - val_loss: 6.7465 - val_acc: 0.1046\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3547 - acc: 0.5245 - val_loss: 6.6544 - val_acc: 0.1266\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3522 - acc: 0.5248 - val_loss: 6.7197 - val_acc: 0.1262\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3604 - acc: 0.5218 - val_loss: 6.6676 - val_acc: 0.1030\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3478 - acc: 0.5232 - val_loss: 6.6531 - val_acc: 0.1008\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3535 - acc: 0.5225 - val_loss: 6.7269 - val_acc: 0.1152\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3532 - acc: 0.5232 - val_loss: 6.6945 - val_acc: 0.1016\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3456 - acc: 0.5267 - val_loss: 6.7171 - val_acc: 0.1014\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3435 - acc: 0.5251 - val_loss: 6.6929 - val_acc: 0.1014\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3464 - acc: 0.5239 - val_loss: 6.6923 - val_acc: 0.0984\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3448 - acc: 0.5260 - val_loss: 6.7110 - val_acc: 0.0990\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3411 - acc: 0.5308 - val_loss: 6.8680 - val_acc: 0.1064\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3435 - acc: 0.5293 - val_loss: 6.8903 - val_acc: 0.0986\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3437 - acc: 0.5283 - val_loss: 6.8303 - val_acc: 0.0986\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3400 - acc: 0.5282 - val_loss: 6.8676 - val_acc: 0.0984\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3381 - acc: 0.5262 - val_loss: 6.8226 - val_acc: 0.0986\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3350 - acc: 0.5314 - val_loss: 6.9379 - val_acc: 0.0986\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3398 - acc: 0.5258 - val_loss: 6.8390 - val_acc: 0.0986\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3328 - acc: 0.5330 - val_loss: 6.9279 - val_acc: 0.0986\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3378 - acc: 0.5282 - val_loss: 6.9374 - val_acc: 0.0986\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3365 - acc: 0.5285 - val_loss: 6.9015 - val_acc: 0.0986\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3297 - acc: 0.5340 - val_loss: 6.9843 - val_acc: 0.0986\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3270 - acc: 0.5284 - val_loss: 6.9596 - val_acc: 0.0986\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3284 - acc: 0.5338 - val_loss: 6.9759 - val_acc: 0.0986\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3226 - acc: 0.5336 - val_loss: 6.9866 - val_acc: 0.0986\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3297 - acc: 0.5324 - val_loss: 7.0945 - val_acc: 0.0986\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 121s 3ms/step - loss: 1.3343 - acc: 0.5325 - val_loss: 7.0861 - val_acc: 0.0986\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3275 - acc: 0.5309 - val_loss: 7.0596 - val_acc: 0.0986\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 120s 3ms/step - loss: 1.3264 - acc: 0.5332 - val_loss: 7.0221 - val_acc: 0.0986\n",
      "Epoch 128/200\n",
      " 9920/45000 [=====>........................] - ETA: 1:28 - loss: 1.3222 - acc: 0.5355"
     ]
    }
   ],
   "source": [
    "number_of_class = 10\n",
    "# x_train: (50000, 32, 32, 3);  y_train: (50000, 1).\n",
    "# x_test: (10000, 32, 32, 3); y_test: (10000, 1).\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Convert classification labels to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, number_of_class)\n",
    "y_test = keras.utils.to_categorical(y_test, number_of_class)\n",
    "\n",
    "# use the resnet trained on Imagenet \n",
    "model_resnet50_conv = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(model_resnet50_conv)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class,activation='softmax'))\n",
    "\n",
    "print('The number of trainable weights before frozen:', len(model.trainable_weights))\n",
    "# avoid the weights from pretrained model are modified during training \n",
    "model_resnet50_conv.trainable = False\n",
    "print('The number of trainable weights after frozen:', len(model.trainable_weights))\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit_model=model.fit(x_train, y_train,\n",
    "                    batch_size=32,epochs=200,verbose=1,\n",
    "                    validation_split=0.10)\n",
    "\n",
    "performance = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable weights before frozen: 30\n",
      "The number of trainable weights after frozen: 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 92s 2ms/step - loss: 1.6670 - acc: 0.4158 - val_loss: 1.3765 - val_acc: 0.5302\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.3898 - acc: 0.5161 - val_loss: 1.2814 - val_acc: 0.5574\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.3111 - acc: 0.5444 - val_loss: 1.2324 - val_acc: 0.5704\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.2623 - acc: 0.5619 - val_loss: 1.2009 - val_acc: 0.5812\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.2280 - acc: 0.5747 - val_loss: 1.1729 - val_acc: 0.5874\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1993 - acc: 0.5859 - val_loss: 1.1604 - val_acc: 0.5944\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1786 - acc: 0.5913 - val_loss: 1.1429 - val_acc: 0.6020\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1620 - acc: 0.5964 - val_loss: 1.1459 - val_acc: 0.6000\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1424 - acc: 0.6033 - val_loss: 1.1336 - val_acc: 0.6000\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1321 - acc: 0.6092 - val_loss: 1.1248 - val_acc: 0.6048\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1203 - acc: 0.6148 - val_loss: 1.1250 - val_acc: 0.6062\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.1094 - acc: 0.6180 - val_loss: 1.1084 - val_acc: 0.6136\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0982 - acc: 0.6225 - val_loss: 1.1046 - val_acc: 0.6156\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0897 - acc: 0.6263 - val_loss: 1.0990 - val_acc: 0.6226\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0799 - acc: 0.6295 - val_loss: 1.0984 - val_acc: 0.6244\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0753 - acc: 0.6300 - val_loss: 1.1016 - val_acc: 0.6176\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0675 - acc: 0.6342 - val_loss: 1.0968 - val_acc: 0.6236\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0612 - acc: 0.6382 - val_loss: 1.0938 - val_acc: 0.6266\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0531 - acc: 0.6410 - val_loss: 1.0931 - val_acc: 0.6240\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 1.0464 - acc: 0.6451 - val_loss: 1.0903 - val_acc: 0.6312\n",
      "The test loss is 1.1260308921813964\n",
      "The test accuracy is 0.6134\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 10\n",
    "# x_train: (50000, 32, 32, 3);  y_train: (50000, 1).\n",
    "# x_test: (10000, 32, 32, 3); y_test: (10000, 1).\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Convert classification labels to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, number_of_class)\n",
    "y_test = keras.utils.to_categorical(y_test, number_of_class)\n",
    "\n",
    "# use the resnet trained on Imagenet \n",
    "model_vgg16_conv = VGG16(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=10)\n",
    "\n",
    "model_vgg = Sequential()\n",
    "model_vgg.add(model_vgg16_conv)\n",
    "model_vgg.add(Flatten())\n",
    "model_vgg.add(Dense(1024,activation='relu'))\n",
    "model_vgg.add(Dropout(0.5))\n",
    "model_vgg.add(Dense(number_of_class,activation='softmax'))\n",
    "\n",
    "print('The number of trainable weights before frozen:', len(model_vgg.trainable_weights))\n",
    "# avoid the weights from pretrained model are modified during training \n",
    "model_vgg16_conv.trainable = False\n",
    "print('The number of trainable weights after frozen:', len(model_vgg.trainable_weights))\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "model_vgg.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit_model=model_vgg.fit(x_train, y_train,\n",
    "                    batch_size=32,epochs=20,verbose=1,\n",
    "                    validation_split=0.10)\n",
    "\n",
    "performance = model_vgg.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
