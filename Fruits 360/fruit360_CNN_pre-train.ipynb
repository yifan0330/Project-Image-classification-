{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import keras \n",
    "import numpy as np\n",
    "import sklearn.model_selection as sk\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout, Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44034 images belonging to 95 classes.\n",
      "Found 4871 images belonging to 95 classes.\n",
      "Found 16421 images belonging to 95 classes.\n"
     ]
    }
   ],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "train_data_dir = '../Fruit360/fruits-360/Training/'\n",
    "test_data_dir = '../Fruit360/fruits-360/Test/'\n",
    "\n",
    "\n",
    "#generators automatically turn image files on disk into batches of processed tensors \n",
    "# use data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.1) #split 10% data as validation dataset \n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "# all labels in training dataset \n",
    "#train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 4.8860 - acc: 0.0176 - val_loss: 4.5423 - val_acc: 0.0563\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 4.4957 - acc: 0.0293 - val_loss: 4.5125 - val_acc: 0.0288\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 4.3496 - acc: 0.0488 - val_loss: 4.3831 - val_acc: 0.0325\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 4.2988 - acc: 0.0410 - val_loss: 4.2509 - val_acc: 0.0813\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 4.2158 - acc: 0.0410 - val_loss: 4.1423 - val_acc: 0.0488\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 4.0838 - acc: 0.0566 - val_loss: 4.0712 - val_acc: 0.0737\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.9661 - acc: 0.0840 - val_loss: 3.9540 - val_acc: 0.0822\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.8105 - acc: 0.0801 - val_loss: 3.7706 - val_acc: 0.1062\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.7594 - acc: 0.0918 - val_loss: 3.5777 - val_acc: 0.1613\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.5770 - acc: 0.1035 - val_loss: 3.4991 - val_acc: 0.1588\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.5105 - acc: 0.1465 - val_loss: 3.4887 - val_acc: 0.1512\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.4267 - acc: 0.1172 - val_loss: 3.3515 - val_acc: 0.1363\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.3885 - acc: 0.1406 - val_loss: 3.2108 - val_acc: 0.2250\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 3.2176 - acc: 0.1621 - val_loss: 3.1476 - val_acc: 0.2087\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.0760 - acc: 0.2129 - val_loss: 3.0400 - val_acc: 0.2238\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.0429 - acc: 0.1875 - val_loss: 2.8996 - val_acc: 0.2550\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 3.0214 - acc: 0.1895 - val_loss: 2.9750 - val_acc: 0.2675\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.8834 - acc: 0.2285 - val_loss: 2.7996 - val_acc: 0.2687\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.8365 - acc: 0.2207 - val_loss: 2.6567 - val_acc: 0.3312\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 2.8483 - acc: 0.2207 - val_loss: 2.5990 - val_acc: 0.3337\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.6531 - acc: 0.2734 - val_loss: 2.5422 - val_acc: 0.3738\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.5289 - acc: 0.2969 - val_loss: 2.4020 - val_acc: 0.3987\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.6530 - acc: 0.2715 - val_loss: 2.4492 - val_acc: 0.3638\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.5428 - acc: 0.2871 - val_loss: 2.2917 - val_acc: 0.4012\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.4665 - acc: 0.2832 - val_loss: 2.1166 - val_acc: 0.4425\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.2896 - acc: 0.3750 - val_loss: 2.1973 - val_acc: 0.4138\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.2984 - acc: 0.3203 - val_loss: 2.0834 - val_acc: 0.4913\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.3307 - acc: 0.3262 - val_loss: 2.0041 - val_acc: 0.4713\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.2292 - acc: 0.3535 - val_loss: 2.0328 - val_acc: 0.4437\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.1708 - acc: 0.3359 - val_loss: 1.9172 - val_acc: 0.5200\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 2.0489 - acc: 0.3984 - val_loss: 1.8072 - val_acc: 0.4968\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 2.0070 - acc: 0.3691 - val_loss: 1.7121 - val_acc: 0.5550\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 2.0053 - acc: 0.3984 - val_loss: 1.6355 - val_acc: 0.5750\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.9336 - acc: 0.4043 - val_loss: 1.7587 - val_acc: 0.5250\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.8351 - acc: 0.4648 - val_loss: 1.6914 - val_acc: 0.5262\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.8489 - acc: 0.4395 - val_loss: 1.4745 - val_acc: 0.5913\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.8260 - acc: 0.4688 - val_loss: 1.5639 - val_acc: 0.5790\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.7600 - acc: 0.4785 - val_loss: 1.4628 - val_acc: 0.6250\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.6227 - acc: 0.5117 - val_loss: 1.5163 - val_acc: 0.5863\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.6228 - acc: 0.5020 - val_loss: 1.3999 - val_acc: 0.6175\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.6508 - acc: 0.4941 - val_loss: 1.2694 - val_acc: 0.6637\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.5603 - acc: 0.5234 - val_loss: 1.2509 - val_acc: 0.6438\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.5781 - acc: 0.5234 - val_loss: 1.3709 - val_acc: 0.6296\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.5723 - acc: 0.5137 - val_loss: 1.3339 - val_acc: 0.6350\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.5370 - acc: 0.5371 - val_loss: 1.2481 - val_acc: 0.6763\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.5112 - acc: 0.5488 - val_loss: 1.0443 - val_acc: 0.7438\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.4931 - acc: 0.4922 - val_loss: 1.2053 - val_acc: 0.6763\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.3845 - acc: 0.5664 - val_loss: 1.1325 - val_acc: 0.7212\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.3544 - acc: 0.5684 - val_loss: 1.1751 - val_acc: 0.7042\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.3903 - acc: 0.5918 - val_loss: 1.1035 - val_acc: 0.7238\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.3537 - acc: 0.5488 - val_loss: 1.1323 - val_acc: 0.6800\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.1954 - acc: 0.6484 - val_loss: 1.0839 - val_acc: 0.6913\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.2361 - acc: 0.5977 - val_loss: 0.9961 - val_acc: 0.7525\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.1669 - acc: 0.6562 - val_loss: 1.0745 - val_acc: 0.7100\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.2807 - acc: 0.5938 - val_loss: 1.0288 - val_acc: 0.7155\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.2315 - acc: 0.6035 - val_loss: 1.1015 - val_acc: 0.7137\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.1676 - acc: 0.6152 - val_loss: 0.8853 - val_acc: 0.8037\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.2113 - acc: 0.5938 - val_loss: 0.9497 - val_acc: 0.7462\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.0974 - acc: 0.6543 - val_loss: 0.9138 - val_acc: 0.7462\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.1805 - acc: 0.6250 - val_loss: 0.8071 - val_acc: 0.8013\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.1003 - acc: 0.6328 - val_loss: 0.9086 - val_acc: 0.7396\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.0663 - acc: 0.6621 - val_loss: 0.8328 - val_acc: 0.7750\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.1144 - acc: 0.6387 - val_loss: 0.8244 - val_acc: 0.7863\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.9278 - acc: 0.7070 - val_loss: 0.8569 - val_acc: 0.7650\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.9693 - acc: 0.6875 - val_loss: 0.8144 - val_acc: 0.7725\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.0529 - acc: 0.6523 - val_loss: 1.0223 - val_acc: 0.7075\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8966 - acc: 0.7051 - val_loss: 0.8160 - val_acc: 0.7800\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.9765 - acc: 0.6816 - val_loss: 0.7936 - val_acc: 0.7813\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 1.0078 - acc: 0.6758 - val_loss: 0.7009 - val_acc: 0.8175\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8966 - acc: 0.7168 - val_loss: 0.7303 - val_acc: 0.8063\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.9636 - acc: 0.6992 - val_loss: 0.7590 - val_acc: 0.7963\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.9234 - acc: 0.6914 - val_loss: 1.0003 - val_acc: 0.7050\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8113 - acc: 0.7285 - val_loss: 0.9025 - val_acc: 0.7425\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8559 - acc: 0.7285 - val_loss: 0.7066 - val_acc: 0.8040\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.8103 - acc: 0.7246 - val_loss: 0.8078 - val_acc: 0.7775\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.8194 - acc: 0.7344 - val_loss: 0.5652 - val_acc: 0.8638\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.8847 - acc: 0.7051 - val_loss: 0.7468 - val_acc: 0.7850\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8635 - acc: 0.7559 - val_loss: 0.7325 - val_acc: 0.7762\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8864 - acc: 0.7109 - val_loss: 0.6858 - val_acc: 0.8087\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7980 - acc: 0.7402 - val_loss: 0.6606 - val_acc: 0.8066\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8273 - acc: 0.7207 - val_loss: 0.6765 - val_acc: 0.8063\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7685 - acc: 0.7500 - val_loss: 0.6354 - val_acc: 0.8225\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.6460 - acc: 0.7754 - val_loss: 0.5779 - val_acc: 0.8337\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7444 - acc: 0.7656 - val_loss: 0.5525 - val_acc: 0.8363\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7124 - acc: 0.7832 - val_loss: 0.6599 - val_acc: 0.8000\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7171 - acc: 0.7676 - val_loss: 0.6950 - val_acc: 0.7914\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8481 - acc: 0.7092 - val_loss: 0.6243 - val_acc: 0.8275\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.7534 - acc: 0.7656 - val_loss: 0.5651 - val_acc: 0.8237\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.7223 - acc: 0.7383 - val_loss: 0.6049 - val_acc: 0.8313\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6969 - acc: 0.7676 - val_loss: 0.6192 - val_acc: 0.8187\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6983 - acc: 0.7773 - val_loss: 0.5293 - val_acc: 0.8300\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6256 - acc: 0.8184 - val_loss: 0.5018 - val_acc: 0.8445\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.7971 - acc: 0.7383 - val_loss: 0.5297 - val_acc: 0.8488\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.7155 - acc: 0.7832 - val_loss: 0.6035 - val_acc: 0.8400\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.8168 - acc: 0.7441 - val_loss: 0.5327 - val_acc: 0.8425\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6269 - acc: 0.8047 - val_loss: 0.6815 - val_acc: 0.8050\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6231 - acc: 0.8066 - val_loss: 0.6520 - val_acc: 0.8037\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6771 - acc: 0.7832 - val_loss: 0.6069 - val_acc: 0.8230\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.6130 - acc: 0.7891 - val_loss: 0.4874 - val_acc: 0.8625\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.7042 - acc: 0.7656 - val_loss: 0.5498 - val_acc: 0.8562\n",
      "The test loss is 0.34790650522336364\n",
      "The test accuracy is 0.91015625\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 95\n",
    "\n",
    "model = Sequential()\n",
    "# kernel_size:(3,3)->specify the height and width of the 2D convolution window\n",
    "#\"SAME\": output size is the same as input size. \n",
    "#This requires the filter window to slip outside input map(need to pad)\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(image_width,image_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Flatten the input. Do not affect the batch size.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# decay: Learning rate decay over each update\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "fit_model=model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=32,validation_steps=50,epochs=100,verbose=1,\n",
    "                    validation_data=validation_generator)\n",
    "performance = model.evaluate_generator(test_generator,steps=32)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlYVVX3wPHvYhBQFBUVZxE1xREUZ80xc6qcy9mybPqV\nZZPNvb7ZaGmWb2WDWk5ZppljOWua8xyi5hRpqBgooiiwf3/ce5HhMilX4N71eR6e4Jx99tmHY4t9\n99lnbTHGoJRSyvm55XcDlFJK3Roa8JVSykVowFdKKRehAV8ppVyEBnyllHIRGvCVUspFaMB3QSLi\nLiJxIlI1L8veQDveFJHpeV3vrSAikSLSPo/r9BARIyKB1p+/FJGXclL2Bs41XESW3Whbs6i3s4gc\nz+t6Vd7wyO8GqOyJSFyqH4sCCUCS9eeHjTGzclOfMSYJ8M3rsipvGWMezIt6RKQmcNgYI6nqngHM\nyIv6VeGhAb8QMMakBFxr7+lBY8zKzMqLiIcxJvFWtE0pVXjokI4TsA6NfCcic0TkIjBERFqKyO8i\nEiMip0Vksoh4WsunHzqYad2/TEQuishmEame27LW/d1E5JCIxIrIxyLym4iMyOF19BKRA9Y2rxaR\n2qn2vSQip0TkgogctA2niEgLEdlp3R4lIu9nUre/iCwVkbMi8q+I/CwilVLt3ygi/xGRTdbrWi4i\npVPtHyEiJ0TknIiMzeIa2ojI3yLilmpbfxHZaf0+0/tip66ZIvJGqp/Hisg/IvI3MDxd2btFZLe1\n7SdF5NVUu9dby8RZv5qKyIMisjZdu7db79tWEWme099NVkSknoiss17vPhHpkWpfTxEJt9YZKSJP\nW7eXs96rGBE5LyLrc3IulT0N+M6jNzAb8AO+AxKB0UAZoDXQFXg4i+MHAa8CpYGTwH9zW1ZEygHz\ngOes5z0GNMtJ40UkGJgJPAGUBVYCP4uIp4jUs7a9sTGmBNDNel6Aj4H3rdtrAj9kcgo34AugKlAN\nuAZ8ZOe6hgMBQDFgjLVtDYBPrPsrARWB8pmc5zdr3e3S1Tvb+n1u7wvWNvS0HtcRuA24M12ROGAI\nlvt/FzDaegzA7WD5pGj92pau7jLAEuADwB+YDCwVkVLpriHD7yabNhcBFlvrLgs8DXwnliEmgGnA\nSGNMcaAhsM66/TngqPWY8lj+rak8oAHfeWw0xvxsjEk2xlw2xmwzxmwxxiQaY44CU0kbhNL7wRiz\n3RhzDZgFhNxA2Z7AbmPMT9Z9E4FzOWz/fcAiY8xq67HvACWA5liCpDdQTyzDVces1wSW4FpLRPyN\nMReNMVvsVW6MOWuMWWD93VwA3iLj7+MrY8xhY0w88H2q6+oPLDTG/GaMSQBeAgQ7jCU51VxgIICI\nlMQSnOda9+f2vtgMsLbvD2PMJeCNdOddbYzZb73/e6zny0m9YPkDccAYM8farplYAm6PVGUy+91k\npTVQBMsf5GvWYchlWO41WO5dXREpbow5b4zZmWp7RaCqMeaqMWZdhprVDdGA7zz+Sv2DiNQRkSXW\nIYALwDgsvcrM/JPq+3iyflCbWdmKqdthDX6ROWi77dgTqY5Nth5byRgTATyD5RrOiGXoytbDvh+o\nC0RYhyK626tcRIqJZdbLSevvYzUZfx85va444HwW1zIb6GsdqukLbDHGRFrbkdv7YpOmDaT6XVnr\nbSkia61DVrHAgzms11b3iXTbTmD5NGOTm38fqes9adJmaExdb2/gbuCkte22YaR3rOVWicifIvJc\nzi5DZUcDvvNIn/b0c2A/UNM63PEamfRK89BpoLLtBxER0gaNrJzCMtRiO9bNWtffAMaYmcaY1kB1\nwB1427o9whhzH1AOy5DEfBHxtlP/89Zjm1l/Hx1zeV1VUrXNF8twll3GmL3WY+4k7XAO3Ph9SdMG\nLENTqc0F5gNVjDF+wJep6s0uJW6a332q+v/OQbuyq7eK9d9Bhnqtn3TuxnLvFnP9U9AFY8zTxphA\noBfwgojk9NOKyoIGfOdVHIgFLlnHx7MdJ84Di4HGInKXiHhgGXMum8Nj5wF3i0h7a8/4OeAisEVE\ngkWkg4h4AZetX0kAIjJURMpYPxHEYgluyXbqL46lZ/qviPhjCbQ59T1wj7UX7QW8SfZBdA6WMeuW\npH2ucKP3ZR7wgPUTQjHg9XT7iwPnjTFXRKQF14dNAM4ARkSCMql7MZbhsnvF8pB+EJbnIUtz2LbM\nbMIyHPeM9VlMR6A7ME9EfERkkIiUsA7hXeT6Pb1LRGpY/1DEWrcnZXIOlQsa8J3XM1gesl3E0qv8\nztEnNMZEAfcCHwLRQA1gF5b3BrI79gCW9n4KnMXyMPNuazDwAt7D8jzgH6AU8Ir10O5AuFhmJ00A\n7jXGXLVzig+xPNCMxhKIcvzSkbXHPhpL0P3b2oZ/sjzI0qvvCPxqjPk31fYbui/GmJ+BKVgebB4C\nfk1X5FHgbevv4SVrW23HXsTyiWiLdeZLWLq6z2IZWnkBy+/naaCnMSarYauctDkBy/OBe7Dcu8nA\nIGPMIWuR4cAJ69DWSGCodXttLENucVgegn9kjNl4M21RFqILoChHERF3LB/r+xljNuR3e5RyddrD\nV3lKRLqKiJ916ONVLB/pt+Zzs5RSaMBXea8Nlil957AMy/SyfrRXSuUzHdJRSikXoT18pZRyEQUq\neVqZMmVMYGBgfjdDKaUKjR07dpwzxuRo+nOBCviBgYFs3749v5uhlFKFhoikf0s6Uw4N+GJJ5Wt7\noSLRGBOW9RFKKaUc5Vb08DsYY3KaQEsppZSD6ENbpZRyEY7u4RvgFxExwOfGmKnpC4jIKGAUQNWq\neb5sqlIqC9euXSMyMpIrV67kd1NUNry9valcuTKennbXy8kRRwf81saYU9aFMX4VkYPGmDSr11j/\nCEwFCAsL05cClLqFIiMjKV68OIGBgaRNaqkKEmMM0dHRREZGUr169ewPyIRDh3SMMaes/z0DLCCH\nqx/lRlTULDZvDmTtWjc2bw4kKipX63kr5dKuXLmCv7+/BvsCTkTw9/e/6U9iDgv41gUnitu+B7pg\nyQOeZ6KiZhERMYqEhBOAISHhBBERozToK5ULGuwLh7y4T47s4QcAG0VkD5bkWUuMMcvz8gRHj75M\ncnJ8mm3JyfEcPfpyXp5GKaWcgsMCvjHmqDGmkfWrnjFmfF6fIyHhZK62K6UKjujoaEJCQggJCaF8\n+fJUqlQp5eerV+0taZDR/fffT0RERJZlpkyZwqxZefOpv02bNuzevTtP6soPBepN29zy8qpqHc7J\nuF0plfeiomZx9OjLJCScxMurKkFB4wkIGHxDdfn7+6cEzzfeeANfX1+effbZNGWMMRhjcHOz3zed\nNm1atud5/PHHb6h9zqhQz8MPChqPm1vRNNvc3IoSFJTnHyaUcnm36pnZkSNHqFu3LoMHD6ZevXqc\nPn2aUaNGERYWRr169Rg3blxKWVuPOzExkZIlSzJ27FgaNWpEy5YtOXPmDACvvPIKkyZNSik/duxY\nmjVrRu3atdm0aRMAly5dom/fvtStW5d+/foRFhaWbU9+5syZNGjQgPr16/PSSy8BkJiYyNChQ1O2\nT548GYCJEydSt25dGjVqxJAhQ/L095UbhbqHb+tZ5FWPQymVuayemeX1/3MHDx5kxowZNG3aFIB3\n3nmH0qVLk5iYSIcOHejXrx9169ZNc0xsbCzt2rXjnXfeYcyYMXz99deMHTs2Q93GGLZu3cqiRYsY\nN24cy5cv5+OPP6Z8+fLMnz+fPXv20Lhx4yzbFxkZySuvvML27dvx8/Ojc+fOLF68mLJly3Lu3Dn2\n7dsHQExMDADvvfceJ06coEiRIinb8kOh7uGDJei3bHmc9u2TadnyuAZ7pRzkVj4zq1GjRkqwB5gz\nZw6NGzemcePGhIeH88cff2Q4xsfHh27dugHQpEkTjh8/brfuPn36ZCizceNG7rvPsu57o0aNqFev\nXpbt27JlCx07dqRMmTJ4enoyaNAg1q9fT82aNYmIiGD06NGsWLECPz8/AOrVq8eQIUOYNWvWTb04\ndbMKfcBXSt0amT0bc8Qzs2LFiqV8f/jwYT766CNWr17N3r176dq1q9356EWKFEn53t3dncTExEza\n65WhTG4XgsqsvL+/P3v37qVNmzZMmTKFhx9+GIAVK1bwyCOPsG3bNpo1a0ZSUlKuzpdXNOArpXIk\nv56ZXbhwgeLFi1OiRAlOnz7NihUr8vwcbdq0Yd68eQDs27fP7ieI1Fq0aMGaNWuIjo4mMTGRuXPn\n0q5dO86ePYsxhv79+zNu3Dh27txJUlISkZGRdOzYkffee49z584RHx+fZf2OUqjH8JVSt05+PTNr\n3LgxdevWpU6dOlSrVo3WrVvn+TmeeOIJhg0bRt26dVO+bMMx9lSuXJlx48bRvn17jDHcdddd9OjR\ng507dzJy5EiMMYgI7777LomJiQwaNIiLFy+SnJzMs88+S/HixfP8GnKiQK1pGxYWZnQBFKVunfDw\ncIKDg/O7GfkuMTGRxMREvL29OXz4MF26dOHw4cN4eBSsPrG9+yUiO3K61kjBuhqllMoHcXFxdOrU\nicTERIwxfP755wUu2OcF57sipZTKpZIlS7Jjx478bobD6UNbpZRyERrwlVLKRWjAV0opF+EUAf/M\niZhcvzihlFKuptAH/KtXEnkq7CueaDSVHyZ+yvq1NXT1K6UKifbt22d4kWrSpEk89thjWR7n6+sL\nwKlTp+jXr1+mdWc3zXvSpElpXoLq3r17nuS6eeONN5gwYcJN15PXCn3AFzfh/vc6kZgUw/Qx5/i4\nV38OrQvW1a+UKgQGDhzI3Llz02ybO3cuAwcOzNHxFStW5Icffrjh86cP+EuXLqVkyZI3XF9BV+gD\nvmcRd+64P4RhX3zKgA+n4VvmAr9OvIur8UV09SulCrh+/fqxePFiEhISADh+/DinTp2iTZs2KXPj\nGzduTIMGDfjpp58yHH/8+HHq168PwOXLl7nvvvsIDg6md+/eXL58OaXco48+mpJe+fXXXwdg8uTJ\nnDp1ig4dOtChQwcAAgMDOXfuHAAffvgh9evXp379+inplY8fP05wcDAPPfQQ9erVo0uXLmnOY8/u\n3btp0aIFDRs2pHfv3vz7778p569bty4NGzZMSdy2bt26lEVgQkNDuXjx4g3/bu1xmnn4V6+epGqo\nocszi5j12MNs/74VrYav1dWvlMqhqU+t4OjuqDytMygkgFGT7sx0v7+/P82aNWP58uXcc889zJ07\nl3vvvRcRwdvbmwULFlCiRAnOnTtHixYtuPvuuzNd2/XTTz+laNGihIeHs3fv3jQpjsePH0/p0qVJ\nSkqiU6dO7N27lyeffJIPP/yQNWvWUKZMmTR17dixg2nTprFlyxaMMTRv3px27dpRqlQpDh8+zJw5\nc/jiiy8YMGAA8+fPzzLH/bBhw/j4449p164dr732Gv/5z3+YNGkS77zzDseOHcPLyytlGGnChAlM\nmTKF1q1bExcXh7e3d25+3dkq9D18G1vGvgrBf1Or7QG2z2tFfGxRXf1KqQIu9bBO6uEcYwwvvfQS\nDRs2pHPnzvz9999ERWX+B2n9+vUpgbdhw4Y0bNgwZd+8efNo3LgxoaGhHDhwINvkaBs3bqR3794U\nK1YMX19f+vTpw4YNGwCoXr06ISEhQNZpmMGSoz8mJoZ27doBMHz4cNavX5/SxsGDBzNz5syUt3pb\nt27NmDFjmDx5MjExMXn+tq/T9PCDgsYTETGK5OR42oxcxZHfgtk6uyOPTr4vv5umVKGQVU/ckXr1\n6sWYMWPYuXMnly9fTumZz5o1i7Nnz7Jjxw48PT0JDAy0mxY5NXu9/2PHjjFhwgS2bdtGqVKlGDFi\nRLb1ZDXrz5ZeGSwplrMb0snMkiVLWL9+PT///DPjx49n3759jB07lh49erB06VJat27NihUrqFOn\nzg3Vb4/T9PADAgZTu/ZUvLyq4V8tmnp3HmTXwsZsWfe4zthRqgDz9fWlffv2PPDAA2ke1sbGxlKu\nXDk8PT1Zs2YNJ05kXL86tdtvvz1lsfL9+/ezd+9ewJJeuVixYvj5+REVFcWyZctSjilevLjdcfLb\nb7+dhQsXEh8fz6VLl1iwYAFt27bN9bX5+flRqlSplE8H3377Le3atSM5OZm//vqLDh068O677xIb\nG0tcXBx//vknDRo04IUXXqBp06YcPHgw1+fMitP08MES9AMCBhMVNYuLI54lfOVtbP6mHXc+u4iI\niFEpZZRSBcvAgQPp06dPmhk7gwcP5q677qJBgwaEhYVl29N99NFHuf/++wkODiY4OJgmTZoAlhWs\nQkNDqVOnDlWqVEmTXnnUqFF069aNChUqsGbNmpTtjRs3ZsSIETRr1gyABx98kNDQ0CyHbzIzY8YM\nHnnkEeLj4wkKCmLatGkkJSUxZMgQYmNjMcbw5JNPUrJkSV599VXWrFmDu7s7devWTVnBK684ZXrk\nzZsDSUg4weqPu7FrYTMemPExpSqfx8urGi1bHr/5hirlJDQ9cuFys+mRnWZIJzXbzJzmgzfgXiSJ\nTTM6WLef0OEdpZTLcsqAb5uZU6x0HI17byF8VQPOHisHoC9kKaVcllMG/NRrbza9dyNFil5l07QO\nKfv1hSylritIw7oqc3lxn5wy4KeesePjd5mw/ps4vKEe/0RUSCmjwztKgbe3N9HR0Rr0CzhjDNHR\n0Tf9IpZTPrRNbfPmQC5E/8MXg5+mXM3T9J8wg9RTdd3cilK79lSdvaNc0rVr14iMjMx2XrrKf97e\n3lSuXBlPT88023VN21SCgsYTcW0UrYavYfXHPTi4qgHBnfel7LcN72jAV67I09OT6tWr53cz1C3i\nlEM6qdmGd5oPiKJ8nUhWT+nG5Qs+acpovh2llCtw+oAPlqDfus0xer60lSsXfFj/eZc0+zXfjlLK\nFbhEwLdp2eVZmt67jX1Lm/DX7kDAMoYfFDQ+fxumlFK3gEsF/ICAwYwYfx8lK15g2bu9iT9fFTc3\nH8LDh+qMHaWU03OpgA9QtfpQXv/paa7GlWbu072JPXMFMPpCllLK6blcwAeoFVaRAe//zIUzxfn+\n2RHEx1pe0tIXspRSzswlAz5AuTrb6P3mbP6NLM2CFweTnGSZnK8zdpRSzsrhAV9E3EVkl4gsdvS5\ncsPLqyrVmhyl6/MLOR1ehb1LmqRsV0opZ3QrevijgfBbcJ5cseXbqdNpH5UbHeO3rzpx9VIpnbGj\nlHJaDg34IlIZ6AF86cjz3AjbC1ne3tXo9MQyrsT58Nu0DjpjRynltBzdw58EPA8kZ1ZAREaJyHYR\n2X727FkHNyetgIDBtGx5nNt7TiDk7l3sXNCAs8fK6owdpZRTcljAF5GewBljzI6syhljphpjwowx\nYWXLlnVUc7J09OjLtBrxC17FEljziWVJMZ2xo5RyNo7s4bcG7haR48BcoKOIzHTg+W5YQsJJfPwu\n03zIek7urMHZPwNStiullLNwWMA3xrxojKlsjAkE7gNWG2OGOOp8N8M2M6fenbtx80hk/4qQNNuV\nUsoZuOw8/NRsM3aK+sVTo+Uhwlc2xCT76owdpZRTuSUB3xiz1hjT81ac60akXiGrXpfdxP9bnGt/\nf6A58pVSTkV7+Fa2GTsjXthPcX83fpn2O2vXuukUTaWU09CAn070+bnc1mELhzdW4XKst07RVEo5\nDQ346Rw9+jL17txG0jUPDq5uAFimaIaHD9HevlKqUNOAn05CwknK1fyHskH/cOCXkHT7tLevlCq8\nNOCnk3qK5j8HKxN9okya/fpCllKqsNKAn871pGp7Ebdkwlc1zFBGX8hSShVGGvDTsU3R9K/oT9XQ\nY4SvaogxacvoC1lKqcJIA74dtimaHYe2IPZUaU6HV07Zp4ueK6UKKw34Weg6YgQeXsKhNa0Bwcur\nGrVrT9UXspRShZJHfjegICvm503zu+qwf50Pr8yehYene343SSmlbpj28LPRfnB9Ys/Gs3vlMaKi\nZrF5c6C+gauUKpS0h5+NsG41KVbSm+Vf/8ztxV4hOTkeuD4nH9AhHqVUoaA9/Gx4ennQpn8wO5b8\ny6V/007X0Tn5SqnCRAN+DvR4LIzkROGn1waSeDXth6KEhBM6xKOUKhQ04OdAUEh57n51PZF7A1n+\nbi9MsqQrYTTtglKqwNOAn0M9Rz1Iu4fXcHB1Q9Z/0ZlrVzwzlNEhHqVUQaYPbXMoIGAwQ98wXIha\nyra5bdk2ty3Fy8VQruY/3PH0z/iWuQho2gWlVMGlPfxcKF9+CG/8MJOXF/Sn3UM7qNLoOMe31WTT\n9A6pShkdz1dKFUjaw88ld3c3WvaqQ1DL/kREjMLL9wp7FjWl+ZD1+JWPAXTKplKqYNIe/g2yJVlr\nM+IY4mbYMvP2NPt10RSlVEGjAf8mBAQM5s67/6BBjx3sXx5K7D8lM5TR2TtKqYJCA34eaDviqKWX\nP6ut3f06e0cpVRBowM8DIS1epmGPPexf1thuLx909o5SKv9pwM8DAQGDGfRqbxDYOqeN3TK6aIpS\nKr9pwM8jwY2H0+WBphxY3oxL0eXS7NNFU5RSBYEG/DzUb2wrkpOEwytewsurGrpoilKqINGAn4fK\nVy9FhyEN2DAznjpB+2nfPpmgoPEcPfqyJlhTSuU7Dfh5bMBLbbh6JZGfJm4hKmoWERGjSEg4gSZY\nU0rlNw34eazSbf60vbceS6ZsZ/+ucSkLptjoFE2lVH7RgO8AA15uw5VLV1nwaguuXs6YVVOnaCql\n8oMGfAcIrF+OJ768ixM7g/j+2RFcvuCTZr9O0VRK5QcN+A7S5YEQHvuyAmcOV2Du6AeIi/YFdIqm\nUir/aMB3oG73j+LpOVW4EFWapW/1pUgRnaKplMo/mh7Zwdr3fZD4M9v532PL+Gd7C0ReJjx8KF5e\nVQkKGq/BXyl1y2gP/xbo+nATajb1YvarJ/k36iy2KZrh4UNZu1Z0fr5S6pbQgH8LuLkJHZ+azdV4\nT9ZM6ZZqjwHQ4K+UuiUcFvBFxFtEtorIHhE5ICL/cdS5CoMSFffSfNAGwlc24tjWmnZKXA/++nKW\nUsoRHNnDTwA6GmMaASFAVxFp4cDzFWheXlVpPng9JStFs+GLOzAm87L6cpZSyhEcFvCNRZz1R0/r\nVxZhzrkFBY2niLcXLYas48yRChzdXDvL8vpyllIqrzl0DF9E3EVkN3AG+NUYs8VOmVEisl1Etp89\ne9aRzclXtjVwQ3rE4FfhPJu/bZeml39wdX3+3Hxbys/6cpZSKq85NOAbY5KMMSFAZaCZiNS3U2aq\nMSbMGBNWtmxZRzYn3wUEDKZN22MM/c9Q/jlYmVN7WgGwb1koi/87gOXv9SLpmru+nKWUcohbMkvH\nGBMDrAW63orzFXSdhjWkTOUS7Jn/ED4XI/j1g96UqhzL5Rhfjv0ehpubD+HhQ3XGjlIqTzlylk5Z\nESlp/d4H6AwcdNT5ChNPLw/6vtCKPzb+xdt9v6dGaAU+2/8mpSq6s2dxHRITo9F0ykqpvObIHn4F\nYI2I7AW2YRnDX+zA8xUqXUaG4F+pOOUCS/LG0oEU8/Om3p3bObatOhei/FLKJSfHEx4+hA0byrBx\nYxldSEUpdcMcllrBGLMXCHVU/YWdl48nk3Y8iHexIvj4FgEguMtaNk5vxP7lobQavjZN+aSk6JTv\nbT1/QFMzKKVyTN+0zUelAnxTgj1AuWolqNbkKPuWNSY5SbI8VufqK6VySwN+ARIUNJ6GPfZzMaok\nJ3cGZVte5+orpXIjRwFfREaLSAmx+EpEdopIF0c3ztUEBAymxwNP41PiCnsWh2VbXufqK6VyI6c9\n/AeMMReALkApYCjwjsNa5cIqVx1Ct4c7cGRjPWJPVcy0nM7VV0rlVk4Dvm1AuTvwrTHmQKptKo/1\nero5Hp7uRCx7Di+vaoDg7u6Ph4c/IHh56UIqSqncy+ksnR0i8gtQHXhRRIoDyY5rlmsrXaE4dzwQ\nwi9f7uKBt/dSpnKJ/G6SUsoJ5LSHPxIYCzQ1xsRjSYR2v8Napej7fCuSkw0LPvg9v5uilHISOQ34\nLYEIY0yMiAwBXgFiHdcsFRBYkg5DGrD88x3Enr2UZl9U1Cw2bw7M8BJWZtuVUgpyHvA/BeJFpBHw\nDPAn8I3DWqUA6De2NVevJPLTpOtJRqOiZhERMYqEhBOkTr9w6NBjdrdr0FdK2eQ04CcaYwxwD/CJ\nMWYKUNxxzVIAVeqUoU3/usx/bzPLp+4E4OjRl0lOjk9TLjk5nlOnptrdri9nKaVscvrQ9qKIvIhl\nOmZbEXHHMo6vHOyJL3oSf/Eqnzy8hL8OnqNGj79wc7dXMsnu8fpyllLKJqc9/HuxLFn4gDHmH6AS\n8L7DWqVSFC3hxWuL7uXu0c34aeIWFr32AAmXvOyUtPtXQF/OUkqlyFHAtwb5WYCfiPQErhhjdAz/\nFnH3cGPUpDt57NPuHN1SlTlPjCLmVKlUJQRLDz/9qxGeJCXF6UNcpRSQ89QKA4CtQH9gALBFRPo5\nsmEqo+6PNOG/vwwh/t8AZj32KH/tDsQS5G1rJRpsQd/d3R8R0dz6SqkUOR3SeRnLHPzhxphhQDPg\nVcc1S2WmUcfqTNz6CP7lKzH/hWFcPJv+2bnBy6saHh6+GHM1zR59iKuUa8tpwHczxpxJ9XN0Lo5V\neaxSLX/+s3wQyUnCtrmtM+xPSDiZ6cPahIQTOryjlIvKadBeLiIrRGSEiIwAlgBLHdcslZ2AwJLU\n7/onexeHcem8b5p9Xl5Vs3xYq8M7SrmmnD60fQ6YCjQEGgFTjTEvOLJhKnv3vdyJpER3ts1rlbLN\nlkUzKGg8bm5FMz1Wh3eUcj05HpYxxsw3xowxxjxtjFngyEapnGnYcgTNepVgz6LmxMcWw8OtOvvn\nv8PYZtEkx3Wjdu2p1myb9ukcfaVcS5YBX0QuisgFO18XReTCrWqkytzw/w4h8Yon4fNnMm/0WJZO\njuHcX7Es+mgrAQGDadnyeKZBX+foK+Vasgz4xpjixpgSdr6KG2M0Z28BULVuWVr1DWbNt/uI/vsi\nrywcQPudu++9AAAgAElEQVQhDVg5bQ+XYq8A2B3e0QVUlHI9OU2toAqwkR/cQeXa/tz1ZDNKlitG\nmcolWPPtPn79eje9nm6RslDK0aMvk5BwEi+vqgQFjc+wgEpU1KxsyyilCi+x5EQrGMLCwsz27dvz\nuxlO4fk204k+dZGphx/H3T37RzW2LJypE7C5uRXVlbWUKuBEZIcxJvtFsNG59E7r7tHNiDoWw7bF\nhzPsS503f8OGMmzcWIbw8CGabVMpJ6cB30m17F2HslVKsOijrWm2p8+nn5QUbU2/YJ/O5FHKeWjA\nd1LuHm70eDyMvWuOc2xvVMp2e/n0s6IzeZRyHhrwnViXB0PxKOLOL1/uStmWux67aCoGpZyIBnwn\nVsK/KK361GH1t/tIuHwNyE2P/XoWTk3FoJRz0IDv5Lo8GMqlmCtsXnAQsD8nPzU3t6J4ePhzPeWy\nRXJyPOHhQ7S3r1QhpgHfyTXsEEhA9ZL88uVuAAICBlOt8mds/KIf50+Wwd3d3xrgBS+vatSuPZXE\nxPOZ1qe9faUKL33xysm5uQldRobw7StrOXXkPAHVS/LtmCL8/lND/v2zG+9vuh83t7QrZVlevjqR\naZ226Zo6P1+pwkV7+C6g04hGuLkJv369mxljV/P7T4do2rMWEVv+ZuX0PRnKZzfsAzpdU6nCSAO+\nCyhTqQRNutdk0aQt/DhhMz3/rymvLbqXum2qMP2FVVw8fzlN+YCAwdlm2tTpmkoVPhrwXUTXh0JJ\nuJxIk241eWhiF0SER6d0I+7fy3z7ypoM5W2ZNoODZ2riNaWchAZ8F9Hsrtt4ddG9jJ3XF3cPy22v\n3jCAnv/XlGWf7eDQtlN2j0vb2xfc3f1xc/MhPHyozthRqpDR5Gku7lLsFR6v/zkAH2x5AP+K6RdF\nv04TrClV8BSI5GkiUkVE1ohIuIgcEJHRjjqXunHF/Lx57ed7uRRzhXE953I57mrKvr/CzxL37/Xx\nfXtpGWzz821J2NauddOev1IFlCOHdBKBZ4wxwUAL4HERqevA86kbFBRSnue/68OxPVG8N/BH1sza\nxzMtv+bRup8x7q7vsH0KzGpmzvUkbEbn6itVQDks4BtjThtjdlq/vwiEA5UcdT51c5p2r8Ujn3Rl\n2+LDfDBkIRejL9NhaAP++O0v1s7aB+RuZo6mVlaq4LklL16JSCAQCmyxs28UMAqgalWd6pefuj8a\nRrGS3hT39yGkcxAAkQej+fq5VTS/uzZBQeMzjOFnxZZ4TVfOUqpgcPgsHRHxBeYDTxljMix8boyZ\naowJM8aElS1b1tHNUdloN7A+jbvUwM1NcHMTHvmkK//+E8fc/27I0fz89HR4R6mCw6EBX0Q8sQT7\nWcaYHx15LuUYtZtV4o4HQvhp0hb+Onguy/n5mdHhHaUKBkfO0hHgKyDcGPOho86jHG/42x3xLubJ\nZ/+3POUBrr35+ZYkbPZpKgal8p/D5uGLSBtgA7APSLZufskYszSzY3QefsG15H/b+fTxZTwzsxcd\nBjfItNzmzYGZJl5zd/dHBBITz+PlVVXH9pXKA7mZh68vXqkcSUpK5rlW0zhzPJbPDj6Kbykfu+Xs\nvZyVOcsiK15e1TT4K3WDCsSLV8q5uLu78X+f9+BCdDzTx67OtFzuHuzqilpK3Uoa8FWOBYWU556n\nmrN86k7CN/2VaTnbg11LDz5n9MGuUo6nAV/lyqA32lG2Sgk+GvkzV+KvZVk2tymUExJOaGoGpRxI\nA77KFR/fIoyedjd/R0Tz1TO/Zlk2JwupZKSpGZRyFA34KtdCOlWnz3MtWfbZDjYvPGi3zC9f7eK/\nXeIo6Tklk6mbWQ/36BCPUnlP17RVN2TIfzuwZ9VxJo9cTK2mFSlTqUTKvgvR8Xz1zK9cik1gQl9f\n3lqzncq1y6Q5PipqlnXt3JPYHt6mp3P3lcpb2sNXN8SziDvPze7N1SuJTBi0gGsJiSn75v53A5cv\nXuWF7/qQnGQY2+4bTv5xNs3xtge77dsnZzqjR5dRVCpvacBXN6zSbf48+WVP9q8/ycQRi0hONvx9\nOJolU7bT5cFQ2g6ox9trhyIivNTx2zS59lOzN9ZvbxnFqKhZbN4cqA92lbpBOqSjbkq7gfU5+9cF\npr+witIVi3PmeAxFvD0Y/J92AFQJLsszM3vxSueZ7Fh+hDb9Mi6JYHvhyjbE4+5eGhEIDx/KoUOj\nrW/nRmN7UQuuz91PfbxSKmvaw1c3re9zLbnriaYs/PB3Nv14kL4vtKJUed+U/Q3aVaOEvw+bF0Rk\nWsf1pGzfYszllMVUri+sAunH+vXBrlK5oz18ddNEhAcnduHi+csc2X6aXmNapNnv7uFGs7tvY/OP\nB7l2NQnPIu6Z1mVvGcWsJCScTPMA2PbpQPP1KJWR9vBVnnB3d+PZmb2ZcuARvIt6ZtjfsldtLsUm\nsG/t8Szryf3MHEN4+FBrwjajSy0qlQUN+CpPubvb/ycVckcQ3sU8Mwzr/BsVl+bnG5uZk3kCQNsi\n6/qQVykN+OoW8fLxpHHXGmz5KYLkZEuAXjjxd4aWn8i6OftTymX/dm7O8/OklpBwgvDwoaxdKxr8\nlcvSgK9umZa9anP+dBwRW/7mwMaTfP3cSjw83fj08WVEn7oIXM+26ekRSNq3cwUvr2oEB3/LjQb9\n1DN8NPgrV6QBX90yTXvUwt3DjRVTd/LuvT8SUL0U72+6n2tXEvlk1BKMMRhj2LGgDhO7PoyJ3E3b\ntudo0+Yc7dsn07LlcQICBufRC1mamlm5Hg346pbxLeVDww6BrJy+h7jzl3nxh77UCqvIsLc7sm3J\nYZZ+uoNJ9y/if48uJTnJ8OWYX4mLuZKhHvvDPpZef3ZLLdqj0zuVq9CAr26pVn3rAPDIlG4ENSoP\nwF1PNKN+u2p8+vgyVs3Yy8DX2vLuhuFcOBfP7DfWZagj/Xq6tqGe9u1NyieC3CyyDpq3R7kGXeJQ\n3VJJickc3f0PtcIqptkedTyGTx5ewl1PNKVZz9sA+OSRJfzy5S4+3jOKavXK5fpc1+fnnyD1W7r2\neHlVsy7akv7YkzqfXxVouqatcgqx5+J5+LYpBIWWZ/zKIYjc6MPa7IJ/2rV1gQzr8rq5FaV27aka\n9FWBowFfOY3FU7bx2f8t56X5/WjVJzhP6sxJ8LfPHUjWHr8qUHQRc+U0uj3chCrBZfjm5TUkJSZn\nWm7Lz4fYvPAgOenA2PL2WJ4BpC+f1fFJ6Bu8qjDTgK8KNHcPN4a+2Z7Ig9GsmbnPbplTR87zdt/v\nGd/7e55p8TX71p3IUd0386BWZ/aowkgDvirwWvauQ62wCsx+Y12ahVZsvn5uJR5F3Bn10Z2cPxXH\ni+2/YfJDi7Pt7d/sfP6s/mBo7n5VEGnAVwWeiDDsrY6cORHL8i92pdm3Z/Uxfl8YwYCX2nD3k834\n/NBj9H6mBb98uYsFH/6eZb05X2TdfnbPzP5gREXNIiJiVEpCNx0CUgWFBnxVKIR0rk6D9tX47s0N\nXLlkWTkrKSmZL57+hXLV/Ljn6eaAJWfPA+93pnW/YKY/v4rdK49mWmfa+fyQPmWDm1tRgoNnEhw8\nI9MVuez15O2leNYkbqog0Fk6qtAI3/QXz7WeTtV6ZQnrXhMRmP/eZl74rg9tB9RLU/Zy3FWeafE1\nMf/EMXH7gwQElsy2/qzm3tvbB/anb2aXz982xRPQuf7qpum0TOW0fvlqF6u/3cfBTX+ReC2Zuq2r\n8O6G4Xbn6J86cp6nm35FUEgAb68ZlmdtSDut0x53LDN6Mufu7o8xl3Wuv7ppuQn4uuKVKlS6jAyl\ny8hQLsdd5eDmSAIblsv0hayKNUvT74VWzHhxNWdOxFCu2vVevjGGpMRkPDwzX33LHtv4fNa9+CSy\ne7M3KSk6w7bUM3+0568cQcfwVaHk41uE0DuCKBXgm2W5tgMsi6Zv/CE8zfbvxm9kROWPOL4vKlfn\nzfkSjIYbSeNse8CrD3yVI2jAV06tfFApajapwMZ5f6RsuxJ/jYUf/k7MmUu82mU2p46cByy9/vXf\nHeCDoQuJPXvJbn25m7tvcHf3z1USN3C3+8BX5/yrvKABXzm9NgPqcmjrKaKOxwCwbvZ+4v69wmOf\ndifpWhKvdJ7JgY0neaP7HN6770fWzNzHq11m2U3NnNu5+0lJ59PNBMqKkNnYf0LCiTQzfHSev7oR\nGvCV02vT35KDZ+P3f2CMYfEn2whsUI5uDzdm3IrBXDx/mRfazuCPjX8x6qM7eX3xfZw8cJbXu83m\nctxVLkTH88N7m3it62wu/vmy3SmameXg9/Kqmi6VQ2ayHvOH68M9hw49psM+6oboLB3lEp5u9hUY\nw8gPuzD29hn839QedH2oMQDhmyNZP2c/fZ9vRZnKJQDYtOAg7/T/gfJBpTj31wWuXknEr2xRYs/G\n02FECZoOm0yyHMt2iqZt1k3Elr9J8ljJuUuPZSjj5uZDYmLGh7i5lT7Fs3INmjxNqXTaDqjL4e2n\nmf7CKoqV9Kb94AYp+4JbVubhyV1Tgj1Aq951GPPNPVy+eJVOIxrxyb6HmXZyNL3GtGDN9AvMefxZ\n6lSLTll20fYSV8KFYJKueeDlVS0l2G/8/g+eazWNOS96ZFi4pXbtqSQmns+Ta7T3fEGHflRq2sNX\nLuHMiRgeCPwYgN7PtGDkhDtuuK7dK4/ydv/5BAT68f6m+/Hy8QQsaR5e7zqbirVK89T0e7itaUW2\nLzvCm/d8h5u7G0mJycw6MwbfUj5p6tu8OTCTOf3Zz+dPzd4iLprX3/kViB6+iHwtImdEZL+jzqFU\nTpWrVpLazSshAj0ey9H/G5kK6RzEs7N6cXR3FP97dCnGGI7vP8Nbfb6nfFAp4i9c5dkWXzP5ocW8\n1ed7qjUoxys/DSApMZltSw5nqM9eTh83t6JUrDgqxzN8bKkeUsssxUP6GT/6KcB1OPLFq+nAJ8A3\nDjyHUjl2/3udOPnHOcoHlbrpupp2r8XA19oyZ9wGytcoxS9f7MKrqCfjfhlMMT8vvhzzK798uYvK\ndfwZt3wQxf2LUrpicTYviKDDkIZp6rL1tu29bOXn1zqbt3pJWakrYxqIEyTEebHglUG0e2QFFeqc\nAixDP5ktAmN7AJy6Xcp5OHRIR0QCgcXGmPo5Ka9DOqowSUpKZlzPuexY/ic+vkV4Z/0waoRWSNl/\nZMdpygX6UcLf0kv/9PFlrJy+h9nnnkkZBsqpnA7PpC+3f3kIy9/tQ707d9Ft7ALAflqH9Gx/RFL/\nETq07DmiT1TmqWl34+Z248tNqrxVIIZ0ckpERonIdhHZfvbs2fxujlI55u7uxrOzetOqbx1eXtA/\nTbAHqNmkQkqwB2jZuzYJ8dfY9UvmGTwzkzazp6R5KJx6SCY8fHiaQH5ovSWp3JHf6pB0zR03t6KI\nkO3bwunf+L1y5QTLPj3G6m/28skzPXTYp5DK94BvjJlqjAkzxoSVLVs2v5ujVK4UL+3DSz/0J6Rz\nULZl67erhm8pbzYvOJiybd3c/Syc+HtKyues2Obzt2+fnDI7KH3u/dQPeRPivDixvQZlqkeREOfD\nqb0tcjErKO0bv+eOlyPunB8+fpdYNSWM35a/pEG/EMr3gK+Uq/DwdKdpz1ps/fkwSYnJzH9/E+8P\nXMCXY37loZpTWDxlG9eu5nxWDmSd2+fPzbVJuuZBxyeX4FXsKpHb+lof2GY3jJvxjd9jW2oB0O+9\nbyhSLIHFb/bgcMSrOWpjVg+F9YHxraUBX6lbqGXvOlw8f5nxfeYx7flVtL23Lm+tGUrFWqX57P+W\n83yb6SReSxtsd688ykcjf7b7KSCr3D6H1tfFt0ws1ULO0KBzItuXnCM+LjKT0pLqvxn/IBzbWosy\nQf8QcNtp7nz2J84cqcDqz2pmG7Dtrf4VHn4/GzeWYe1aITx8qL4xfAs5clrmHGAzUFtEIkVkpKPO\npVRh0fjOGnj5eLD158N0eTCUZ2f1pmH7QN5ZN4ynp9/N4W2n+PH9zSnlY85c4r2BC/j169283X9+\nhj8GmeX2uRrvw7EttQju+Bd1gqdSvfVGEuK8ObEz49CTl1c1goO/tT4fyBjsr8YX4e99Vane9AgA\nNVtHULfLLnbMb8neHU9kGbDtfwK5lurN4rTn00RxjuWwgG+MGWiMqWCM8TTGVDbGfOWocylVWHgX\n9WTwf9szbHwHnpjaA3d3y/+CIkKn4Y1o0z+YOePWExlxDmMM/3tsKfEXEuj3Qit2LDvC5AfTLs5u\nm8N/6bwvB1fXJ+GSF25uRbny10SSrnnS74n3CQgYTKVGv1Ok2BUOra2XrkWS8jwgs08LJ3cGkZzo\nQfXm198hCL1nG0lXPYhYnzY/kG0pxw0byrBxY5ksp5NmxjZtNC+Gem7FkFFhGpbSBVCUusX6PNMy\n030PT+7Krl+P8cmoJXR9uDGb5h9k+Nsd6T+2NV7FPJn12jp8fIvQfkgDAgL9MFe6s/Xrt9gw5zxJ\nVz0oXvYSQ98JYv8vpShdIZ7gVlUAKFa8EjVbHbTM1kl0w90jGUj7CcHLq6rdAH1yZyhexYSgJpCY\nDCCUD47Er+J5Dq5qSP2uuzMcY2+Bl5wzhIcP5WbfDUg/RfVm3zHIyTKXBf09Bh3DV6oAKVXel5Ef\n3MH+9SeZOHwRtZtXos+zlj8Q973Slh6Ph7Hkf9t5rtU0hlWcxINBn7BxziU6D2/Kqz8NoEzFIP43\nMopNPx6kZZ86KfPlg4LGU7vDEa5cLMrWOW25HOuT4e1ce2/8ihTlxLYGhHauRZvbj6YM+4hAcMd9\nnNgZxKXzxVLKJye5EXM6+/WDs5dxqCe3i8Dn9E3jnLD3LCIiYhSHD48uVOsXaA9fqQLmjvsbsXbW\nPg5uiuSp6Xfj7nF92OeRj7vS8/Ew/jkaw5kTsVyOu0r7wfUpU8mS+C2sey1++mgLS/+3gy4jQ1Lq\nDAgYzJ2Dktn8zTZ++7oTm2Z0ILhNMSp+2JGAgOtlIO0bv96JrxMdGcm9L9cE0j4krtNxL7/PbEfE\nuno07r0VgJWTenBgRSgPz5tA0ZIZZw+5u/uTnHwRY1I/gM4+NbRN6h50+ramXwoysyEq29oCuVk6\nMrM/HpnNkMrdQjm3jiZPU6oAuhJ/jZioOMpXv/k0EKkZY/hz1z/89kM4K6ftwd3Djcm7H0rzglhq\nCyf+zpdjfuWrY08QEFgyQ6K3GSMfw9PnKoM++ZJjW2sy/wXLYvE9Xv6e4M770tRlS+5mb2gk9RBO\nTuRkEfjMk9LZWP7QpE9NYc/atW65at+tTFWdmzdtNeAr5aL+3HWaZ1pMI7RLEK8tujfNYvAxZy6x\n4oud/Dx5G76lvfks/DEg47j4ltlt2fDFHTyz8CyfP+SFV9ErxMcUo3qzw3R/6ceU+hIT/Ija/ha3\nhd7BbU0r4le2WJq2ZB+cc87d3R8RrDOBcvrpwVLu+rHn03xqyLp9ac+Rm4yk9v745XbsPzcBX4d0\nlHJRNUIrMHJCZz5/cgU/TdpCr6dbcGjbKX6evJUN8/4g8WoSIZ2rM+ytjinHpB/2adj1PBu+gC8e\nrEb8v/H0f2cNv88J5Pj223B38ycp2RI4T659jnn/iQbmAlChRimGju/A7fdaZg0FBY3PkCsoN0M9\nqaV9YGxbTD67ekyGY1MPH9lvX8ZzZPZpoaA88NUevlIuzBjD+D7fs33JYaqHlOfwtlP4FC9C5xGN\n6PF4GJVrl8m2judaTyN8UyT3vdqWIePas2bmXj4Y+hOTtj9IzSYVMMbwfw0/p4iPJw+835nD206x\n4bsDHN5+mrYD6vLIlG74lSmao6AIOVslLPGqB0lX3fHyTbBuyd3aAqllHIqy39PPbBgns8R3mV1D\nboeDtIevlMoREWH0V3cxptlXXIq5wsMfd6XT8IYULe6V4zr6v9iadbP3c+8rbQEI7VIDgB3Lj1Cz\nSQWO7DjNif1nefyz7jRoV40G7apxz1PNmf/eJma/sY59605w9+hmdBx6Fy1bWnq2F89f5s9d/xAU\nEkDt2tc/Ubi7l85yuCbxqgd7FoWxZXZbjBHunTiNMoFngSTc3IpmmzTOHtsDWNvKZpmN59vKrZu7\nHwy06huMZxH3AvXAV3v4SimSkpJxc5M04/g346mwLyni48F7G0bw6f8t49evdvPN6afxLemdptzR\nPf8wdfQv7F93AhGo17YqF87Fc/KPcwD4lvJm+Nsd6fJgKOfOzcly2OfIb7VZOakncef8qBJ6lPMn\ny4AR7vvoK8rXKJ4q3XPaNQCy5w4kp3zqyKyX7+VVjeLub/BiqxOYJDd8/eMI7bWPBneto6hfzv/Q\nOLKHr/PwlVK4u7vlWbAHaNK1Bgc3R/JvVBzrZu+nZe/aGYI9QFCj8ryzdhhfHHmc+167nUuxCZSt\nVpJh4zvwysIBVG8UwJRHlvJM86/ZvOIDO71iy4PWuLPlWfJmP3z84hnw4TTu/XA6/SfMIDlZmDfm\nAYq7v5Eq26hJlUoCrucRykwSqefe+/t3t7tCmb9/d+a9uwCRZLq/9ANlapxmw1ctmTbsCcJXNiAn\nfWt7K5flJe3hK6Xy3IGNJ3mh7QzaDqjLhnl/MG7FIBpbh3pywxjD+rkH+HLMr1yMOU+Pl3+gVpuD\n6coIv771PuG/xTLi648pXck9ZaZNzMkQZo/uS7ESxfnPsoFUrVs2Td0n/zjL9l9/ZM/69Vw4C12f\n24Bv6cvWFNJu2Bv3tz2Y/fPIy1y9dv15w/5d4/i49wBqt99PtxcWAnD2aDl+mXAPp8OrUKNVOHeM\n+Rlf/zi715qT6aH26LRMpVS+SkpMZlCZCVyKTaBM5RJ8dfyJlLxBN+L86YuM7fwSp8LLcPtDK2l6\n30ZsH0j+3NCeBa915KGJXbjnqeYZjv1z12ne6D6Xq5ev8dKP/WnUsTrH9kbx+ZMr2L/OMjTjU7wI\n1xKSqNe2KuNWDMLd3S2LufdCFb+/eb3rHNoPacDICZ0REd5+oAO/TevIiK8/pkz164s5JScJO+a3\n5LevOuFX8TzDv/wUN/fkDHW2b59+W87okI5SKl+5e7gRcoclM2fHYQ1uKtgDlK5QnFeXNKNOh4Os\nn9qFeWPu549fG3LpXACrJt9BraYV6flEU7vH1gitwAe/349/5RK8duds3ur7PaNDv+Dk/jM8NLEL\nn4Y/yncxz/PY/7qxZ9Ux5v53A5B5JtK4qPq8dudsLl9MYOGHvzPjxdVcib/GrgUtCWoZkSbYA7i5\nG5oO2ESPV78n+ngAexZljM2ZnSuv6SwdpZRDtOpThy2LDtH5/pDsC+dAlcChPDtL+O6dOWyddxtL\n3+qHCIhbMm+u7JHlH5Vy1Ury3sYRvN3vB37/KYIej4cx6I12FC/tk1LmjgdCOLDhJHPHrSe4VWWC\nGmWce3/pXADznhkIwEe7HuKnSVv54d1N7F9/kvgYH5oPzHyE4ra2J6nVIoHfpneiTqd9+JS4DMCR\njSHEHBlBixYmT5+j2KNDOkophzDGcPH85UzTNtyM5GTDHxtPsmbmPmo0rkD3R5rk6LikpGTizl/O\n8KavzZVLVxnT/Gtioi4xckJnbmu3l8jTrxIf9zfHt7Rh07Q7uXBWeHvtMGo2rkBysmHyyJ9ZOX0P\ntZtX4rmFRTh27JV0U0ivv7UbH9WZJ0OnEtZvP+0e/Z4Dy+5g+YQ21G5emf/+Mhgf3yK5/l3oGL5S\nSt2gyIhzvNlrHpEHoylW0puw7jXZs+oYMVGXKFfNjzHf9qJ+2+tDMElJySz88HdCuwQR1Kh8tvV/\n8vASfv16N10fbsySKdsJ616Tsd/3w7uo5w21VwO+UkrdBGMM+9adYPnnO9m25DAN2lej2yNNaHxn\njZt+HhFz5hKjak0h/kICHYY2YPRXd+Hh6X7D9WnAV0qpAmzLz4c4dfg89zzVPGXNghulqRWUUqoA\na37XbflyXp2WqZRSLkIDvlJKuQgN+Eop5SI04CullIvQgK+UUi5CA75SSrkIDfhKKeUiNOArpZSL\nKFBv2orIWcD+CsH2lQHOOag5BZUrXjO45nW74jWDa173zVxzNWNM2eyLFbCAn1sisj2nrxQ7C1e8\nZnDN63bFawbXvO5bdc06pKOUUi5CA75SSrmIwh7wp+Z3A/KBK14zuOZ1u+I1g2te9y255kI9hq+U\nUirnCnsPXymlVA5pwFdKKRdRKAO+iHQVkQgROSIiY/O7PY4iIlVEZI2IhIvIAREZbd1eWkR+FZHD\n1v+Wyu+25jURcReRXSKy2PpzdRHZYr3m70Qk96s9F3AiUlJEfhCRg9Z73tLZ77WIPG39t71fROaI\niLcz3msR+VpEzojI/lTb7N5bsZhsjW97RaRxXrWj0AV8EXEHpgDdgLrAQBGpm7+tcphE4BljTDDQ\nAnjceq1jgVXGmFrAKuvPzmY0EJ7q53eBidZr/hcYmS+tcqyPgOXGmDpAIyzX77T3WkQqAU8CYcaY\n+oA7cB/Oea+nA13Tbcvs3nYDalm/RgGf5lUjCl3AB5oBR4wxR40xV4G5wD353CaHMMacNsbstH5/\nEUsAqITlemdYi80AeuVPCx1DRCoDPYAvrT8L0BH4wVrEGa+5BHA78BWAMeaqMSYGJ7/XWJZZ9RER\nD6AocBonvNfGmPXA+XSbM7u39wDfGIvfgZIiUiEv2lEYA34l4K9UP0datzk1EQkEQoEtQIAx5jRY\n/igA5fKvZQ4xCXgeSLb+7A/EGGMSrT874z0PAs4C06xDWV+KSDGc+F4bY/4GJgAnsQT6WGAHzn+v\nbTK7tw6LcYUx4Ntb4t2p55aKiC8wH3jKGHMhv9vjSCLSEzhjjNmRerOdos52zz2AxsCnxphQ4BJO\nNHxjj3XM+h6gOlARKIZlOCM9Z7vX2XHYv/fCGPAjgSqpfq4MnMqntjiciHhiCfazjDE/WjdH2T7i\nWf97Jr/a5wCtgbtF5DiW4bqOWHr8Ja0f+8E573kkEGmM2WL9+QcsfwCc+V53Bo4ZY84aY64BPwKt\ncDZin5YAAAMYSURBVP57bZPZvXVYjCuMAX8bUMv6JL8Iloc8i/K5TQ5hHbv+Cgg3xnyYatciYLj1\n++HAT7e6bY5ijHnRGFPZGBOI5d6uNsYMBtYA/azFnOqaAYwx/wB/iUht66ZOwB848b3GMpTTQkSK\nWv+t267Zqe91Kpnd20XAMOtsnRZArG3o56YZYwrdF9AdOAT8Cbyc3+1x4HW2wfJRbi+w2/rVHcuY\n9irgsPW/pfO7rQ66/vbAYuv3QcBW4AjwPeCV3+1zwPWGANut93shUMrZ7zXwH+AgsB/4FvByxnsN\nzMHynOIalh78yMzuLZYhnSnW+LYPyyymPGmHplZQSikXURiHdJRSSt0ADfhKKeUiNOArpZSL0ICv\nlFIuQgO+Ukq5CA34St0EEWlvy+ipVEGnAV8ppVyEBnzlEkRkiIhsFZHdIvK5Nd9+nIhMtOZjXyUi\nZa1lQ0Tkd2su8gWp8pTXFJGVIrJHRHaKSA1r9b6p8tjPsr41ioi8IyJ/WOuZkE+XrlQKDfjK6YlI\nMHAv0NoYEwIkAYOxJOvaboypB6wDXrce8g3wgjGmIZY3HW3bZwFTjDGNsOR8sb3uHgo8hWV9hiCg\ntYiUBnoD9az1vOnYq1QqexrwlSvoBDQBtonIbuvPQVjSL39nLTMTaCMifkBJY8w66/YZwO0iUhyo\nZIxZAGCMuWKMibeW2WqMiTTGJGNJfxEIXACuAF+KSB/AVlapfKMBX7kCAWYYY0KsX7WNMW/YKZdV\nnhF7KWttElJ9nwR4GEs+92ZYMp32BJbnss1K5TkN+MoVrAL6iUg5SFlLtBqWf/+2rIyDgI3GmFjg\nXxFpa90+FFhnLOsQRIpIL2sdXiJSNLMTWtcw8DPGLAWexrJkoVL5yiP7IkoVbsaYP0TkFeAXEXHD\nkrHwcSyLjDSz7juDZZwfLKlqP7MG9KPA/dbtQ4HPRWSctY7+WZy2OPCTiHhj+XQwJo8vS6lc02yZ\nymWJSJwxxje/26HUraJDOkop5SK0h6+UUi5Ce/hKKeUiNOArpZSL0ICvlFIuQgO+Ukq5CA34Sinl\nIv4fNJMrxPMc4OsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53be7f1630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = fit_model.history['loss']\n",
    "val_loss = fit_model.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'yo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'indigo', label='Validation loss')\n",
    "plt.title('Training loss and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB50vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53be7f1588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = fit_model.history['acc']\n",
    "val_acc = fit_model.history['val_acc']\n",
    "plt.plot(epochs, acc, 'yo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'indigo', label='Validation accuracy')\n",
    "plt.title('Training accuracy and validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 56s 2s/step - loss: 4.5648 - acc: 0.0078 - val_loss: 4.5337 - val_acc: 0.0262\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 55s 2s/step - loss: 4.4935 - acc: 0.0273 - val_loss: 4.4216 - val_acc: 0.0250\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 53s 2s/step - loss: 4.2934 - acc: 0.0352 - val_loss: 4.2727 - val_acc: 0.0392\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 4.1399 - acc: 0.0625 - val_loss: 4.0725 - val_acc: 0.0925\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.8676 - acc: 0.0742 - val_loss: 3.8475 - val_acc: 0.0912\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.8077 - acc: 0.0898 - val_loss: 3.6875 - val_acc: 0.1037\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.7493 - acc: 0.0781 - val_loss: 3.5947 - val_acc: 0.1350\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.5355 - acc: 0.1016 - val_loss: 3.5260 - val_acc: 0.0975\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.4432 - acc: 0.1250 - val_loss: 3.3447 - val_acc: 0.1669\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.3985 - acc: 0.1270 - val_loss: 3.2653 - val_acc: 0.1800\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.1663 - acc: 0.1465 - val_loss: 3.1892 - val_acc: 0.1837\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.2280 - acc: 0.1523 - val_loss: 3.1275 - val_acc: 0.1725\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.1606 - acc: 0.1406 - val_loss: 3.0444 - val_acc: 0.2075\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.0486 - acc: 0.1699 - val_loss: 2.8616 - val_acc: 0.2437\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.9752 - acc: 0.1621 - val_loss: 2.9052 - val_acc: 0.2617\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.9373 - acc: 0.1699 - val_loss: 2.8073 - val_acc: 0.2800\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.8022 - acc: 0.2148 - val_loss: 2.7230 - val_acc: 0.2875\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.7364 - acc: 0.2500 - val_loss: 2.7245 - val_acc: 0.2712\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.7551 - acc: 0.2246 - val_loss: 2.6604 - val_acc: 0.3187\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.7130 - acc: 0.2617 - val_loss: 2.6434 - val_acc: 0.2938\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.6136 - acc: 0.2871 - val_loss: 2.5548 - val_acc: 0.2819\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.6002 - acc: 0.2656 - val_loss: 2.5363 - val_acc: 0.3387\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.4587 - acc: 0.2988 - val_loss: 2.2808 - val_acc: 0.4125\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.3964 - acc: 0.2969 - val_loss: 2.2373 - val_acc: 0.3912\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.3864 - acc: 0.2812 - val_loss: 2.1484 - val_acc: 0.4350\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.3393 - acc: 0.2949 - val_loss: 2.0732 - val_acc: 0.4625\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.2084 - acc: 0.3633 - val_loss: 1.9627 - val_acc: 0.4550\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.1777 - acc: 0.3496 - val_loss: 2.1404 - val_acc: 0.4159\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.9949 - acc: 0.4023 - val_loss: 1.8039 - val_acc: 0.5138\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 2.1419 - acc: 0.3750 - val_loss: 1.8127 - val_acc: 0.5600\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 2.0658 - acc: 0.3848 - val_loss: 1.7337 - val_acc: 0.5275\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.9516 - acc: 0.3965 - val_loss: 1.6311 - val_acc: 0.5600\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.7993 - acc: 0.4688 - val_loss: 1.5350 - val_acc: 0.5537\n",
      "Epoch 34/100\n",
      "31/32 [============================>.] - ETA: 1s - loss: 1.7652 - acc: 0.4294Epoch 37/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.5895 - acc: 0.5059 - val_loss: 1.2683 - val_acc: 0.6288\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.6667 - acc: 0.4863 - val_loss: 1.7755 - val_acc: 0.4788\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.5200 - acc: 0.5039 - val_loss: 1.2661 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.4968 - acc: 0.5254 - val_loss: 1.2493 - val_acc: 0.6157\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.3880 - acc: 0.5586 - val_loss: 1.3304 - val_acc: 0.6250\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.4450 - acc: 0.5176 - val_loss: 1.1455 - val_acc: 0.6737\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.4355 - acc: 0.5703 - val_loss: 1.1416 - val_acc: 0.6625\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.3393 - acc: 0.5625 - val_loss: 1.1300 - val_acc: 0.6562\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.3381 - acc: 0.5566 - val_loss: 1.3587 - val_acc: 0.5725\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.3237 - acc: 0.5898 - val_loss: 1.2930 - val_acc: 0.6207\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.2608 - acc: 0.5898 - val_loss: 1.0614 - val_acc: 0.6887\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.1996 - acc: 0.6211 - val_loss: 1.0993 - val_acc: 0.6750\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.2310 - acc: 0.6035 - val_loss: 0.9444 - val_acc: 0.7075\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.1588 - acc: 0.6504 - val_loss: 1.1975 - val_acc: 0.6587\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.1076 - acc: 0.6504 - val_loss: 0.9371 - val_acc: 0.7163\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.1973 - acc: 0.6328 - val_loss: 0.8258 - val_acc: 0.7358\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.0315 - acc: 0.6699 - val_loss: 1.1292 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.9895 - acc: 0.6504 - val_loss: 0.8293 - val_acc: 0.7100\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.0506 - acc: 0.6719 - val_loss: 0.9786 - val_acc: 0.7113\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.0918 - acc: 0.6562 - val_loss: 0.8696 - val_acc: 0.7338\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.0141 - acc: 0.6797 - val_loss: 1.0061 - val_acc: 0.7150\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.0818 - acc: 0.6484 - val_loss: 0.7530 - val_acc: 0.7876\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.0403 - acc: 0.6406 - val_loss: 0.8957 - val_acc: 0.7312\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9443 - acc: 0.6797 - val_loss: 0.7960 - val_acc: 0.7675\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 1.1037 - acc: 0.6562 - val_loss: 0.8956 - val_acc: 0.7100\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9859 - acc: 0.7012 - val_loss: 0.8917 - val_acc: 0.7412\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9096 - acc: 0.6953 - val_loss: 0.7217 - val_acc: 0.8013\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 1.0048 - acc: 0.6797 - val_loss: 0.9340 - val_acc: 0.7219\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9235 - acc: 0.7227 - val_loss: 0.7080 - val_acc: 0.8000\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.8654 - acc: 0.7227 - val_loss: 0.8533 - val_acc: 0.7312\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9035 - acc: 0.7031 - val_loss: 0.7556 - val_acc: 0.7650\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9232 - acc: 0.6992 - val_loss: 0.6941 - val_acc: 0.7863\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.7985 - acc: 0.7637 - val_loss: 0.7225 - val_acc: 0.7575\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.9048 - acc: 0.6953 - val_loss: 0.7699 - val_acc: 0.7965\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.8765 - acc: 0.7402 - val_loss: 0.7300 - val_acc: 0.7925\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9437 - acc: 0.7012 - val_loss: 0.9517 - val_acc: 0.7125\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.8667 - acc: 0.7188 - val_loss: 0.7262 - val_acc: 0.7675\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.8537 - acc: 0.7129 - val_loss: 0.8095 - val_acc: 0.7588\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7691 - acc: 0.7559 - val_loss: 0.9069 - val_acc: 0.7375\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.8553 - acc: 0.7559 - val_loss: 0.6577 - val_acc: 0.7775\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7727 - acc: 0.7500 - val_loss: 0.6719 - val_acc: 0.8075\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.9110 - acc: 0.7188 - val_loss: 0.7615 - val_acc: 0.7850\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.8546 - acc: 0.7305 - val_loss: 0.5972 - val_acc: 0.8125\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.8203 - acc: 0.7500 - val_loss: 0.7028 - val_acc: 0.7925\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.7294 - acc: 0.7637 - val_loss: 0.5637 - val_acc: 0.8350\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.7972 - acc: 0.7463 - val_loss: 0.5995 - val_acc: 0.8129\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.6659 - acc: 0.7832 - val_loss: 0.7106 - val_acc: 0.7612\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7884 - acc: 0.7773 - val_loss: 0.5622 - val_acc: 0.8325\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7921 - acc: 0.7266 - val_loss: 0.6738 - val_acc: 0.8025\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.6948 - acc: 0.7773 - val_loss: 0.6307 - val_acc: 0.8025\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.6517 - acc: 0.7793 - val_loss: 0.6270 - val_acc: 0.8175\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7213 - acc: 0.7637 - val_loss: 0.6666 - val_acc: 0.7950\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.6432 - acc: 0.7734 - val_loss: 0.9137 - val_acc: 0.7547\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.6808 - acc: 0.7891 - val_loss: 0.8209 - val_acc: 0.7762\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7784 - acc: 0.7363 - val_loss: 0.5750 - val_acc: 0.8037\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7374 - acc: 0.7969 - val_loss: 0.6131 - val_acc: 0.8225\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.6395 - acc: 0.7930 - val_loss: 0.4655 - val_acc: 0.8575\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.7862 - acc: 0.7578 - val_loss: 0.4424 - val_acc: 0.8625\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.6562 - acc: 0.8027 - val_loss: 0.7073 - val_acc: 0.7826\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.7092 - acc: 0.7930 - val_loss: 0.6459 - val_acc: 0.7987\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.7673 - acc: 0.7559 - val_loss: 0.5057 - val_acc: 0.8650\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.5412 - acc: 0.8184 - val_loss: 0.5479 - val_acc: 0.8213\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.6443 - acc: 0.7910 - val_loss: 0.9357 - val_acc: 0.7575\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.7115 - acc: 0.7715 - val_loss: 0.6014 - val_acc: 0.8213\n",
      "The test loss is 0.2821276366012171\n",
      "The test accuracy is 0.921875\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 95\n",
    "\n",
    "model = Sequential()\n",
    "# kernel_size:(3,3)->specify the height and width of the 2D convolution window\n",
    "#\"SAME\": output size is the same as input size. \n",
    "#This requires the filter window to slip outside input map(need to pad)\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(image_width,image_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Flatten the input. Do not affect the batch size.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# decay: Learning rate decay over each update\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "fit_model=model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=32,validation_steps=50,epochs=100,verbose=1,\n",
    "                    validation_data=validation_generator)\n",
    "performance = model.evaluate_generator(test_generator,steps=32)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable weights before conv_base is frozen: 36\n",
      "The number of trainable weights after conv_base is frozen: 4\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.6238 - acc: 0.0234 - val_loss: 4.4180 - val_acc: 0.0488\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.5178 - acc: 0.0156 - val_loss: 4.3523 - val_acc: 0.0663\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.4927 - acc: 0.0176 - val_loss: 4.3452 - val_acc: 0.0455\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.4150 - acc: 0.0430 - val_loss: 4.2670 - val_acc: 0.0887\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.3431 - acc: 0.0625 - val_loss: 4.1682 - val_acc: 0.1550\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.2856 - acc: 0.0684 - val_loss: 4.1371 - val_acc: 0.1138\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.2296 - acc: 0.0801 - val_loss: 4.0697 - val_acc: 0.1700\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.1877 - acc: 0.1016 - val_loss: 4.0782 - val_acc: 0.1675\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.1499 - acc: 0.0996 - val_loss: 3.9627 - val_acc: 0.1681\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.0797 - acc: 0.1113 - val_loss: 3.9577 - val_acc: 0.1988\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.0918 - acc: 0.1074 - val_loss: 3.8790 - val_acc: 0.1638\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.0161 - acc: 0.1387 - val_loss: 3.8430 - val_acc: 0.2025\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.0134 - acc: 0.1230 - val_loss: 3.8267 - val_acc: 0.1862\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 4.0189 - acc: 0.1309 - val_loss: 3.7659 - val_acc: 0.2188\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 3.9045 - acc: 0.1484 - val_loss: 3.6988 - val_acc: 0.2200\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.9025 - acc: 0.1465 - val_loss: 3.7051 - val_acc: 0.2300\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.6592 - acc: 0.1895 - val_loss: 3.5709 - val_acc: 0.2537\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.7882 - acc: 0.1582 - val_loss: 3.5421 - val_acc: 0.2950\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.7387 - acc: 0.1699 - val_loss: 3.5742 - val_acc: 0.2662\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.7829 - acc: 0.1406 - val_loss: 3.5417 - val_acc: 0.2800\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.6179 - acc: 0.2031 - val_loss: 3.3779 - val_acc: 0.2882\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.5624 - acc: 0.1953 - val_loss: 3.4033 - val_acc: 0.3038\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.6223 - acc: 0.1895 - val_loss: 3.3454 - val_acc: 0.3212\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.5517 - acc: 0.1816 - val_loss: 3.2981 - val_acc: 0.3275\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.4730 - acc: 0.1934 - val_loss: 3.2686 - val_acc: 0.3475\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.4702 - acc: 0.1895 - val_loss: 3.2687 - val_acc: 0.3325\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.5067 - acc: 0.2070 - val_loss: 3.1984 - val_acc: 0.3475\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 3.4950 - acc: 0.1973 - val_loss: 3.1634 - val_acc: 0.3755\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.4453 - acc: 0.2207 - val_loss: 3.0774 - val_acc: 0.3588\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.3825 - acc: 0.2148 - val_loss: 3.1120 - val_acc: 0.3787\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.3060 - acc: 0.2539 - val_loss: 3.0545 - val_acc: 0.3600\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.2688 - acc: 0.2363 - val_loss: 3.0755 - val_acc: 0.3912\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.1478 - acc: 0.2637 - val_loss: 3.0188 - val_acc: 0.3412\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 3.2005 - acc: 0.2656 - val_loss: 2.9464 - val_acc: 0.3919\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.1396 - acc: 0.2832 - val_loss: 2.9973 - val_acc: 0.3738\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.0877 - acc: 0.2832 - val_loss: 2.8662 - val_acc: 0.4400\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.1841 - acc: 0.2832 - val_loss: 2.8967 - val_acc: 0.3625\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.1000 - acc: 0.2949 - val_loss: 2.8523 - val_acc: 0.4037\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.0680 - acc: 0.2969 - val_loss: 2.8057 - val_acc: 0.4437\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 3.0177 - acc: 0.2988 - val_loss: 2.8127 - val_acc: 0.3957\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.0984 - acc: 0.2773 - val_loss: 2.7368 - val_acc: 0.4688\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.0895 - acc: 0.2617 - val_loss: 2.6673 - val_acc: 0.4850\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 3.0345 - acc: 0.2852 - val_loss: 2.7353 - val_acc: 0.4625\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.8560 - acc: 0.3359 - val_loss: 2.6130 - val_acc: 0.4512\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.9096 - acc: 0.2969 - val_loss: 2.6917 - val_acc: 0.4412\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 2.9484 - acc: 0.3145 - val_loss: 2.5929 - val_acc: 0.4678\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.8752 - acc: 0.3301 - val_loss: 2.5880 - val_acc: 0.4475\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.8259 - acc: 0.3496 - val_loss: 2.5391 - val_acc: 0.4700\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.9014 - acc: 0.3379 - val_loss: 2.5532 - val_acc: 0.4988\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.8187 - acc: 0.3281 - val_loss: 2.5255 - val_acc: 0.4950\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.7317 - acc: 0.3555 - val_loss: 2.4984 - val_acc: 0.4850\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.7208 - acc: 0.3438 - val_loss: 2.4503 - val_acc: 0.5057\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.7235 - acc: 0.3691 - val_loss: 2.4189 - val_acc: 0.5487\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.6022 - acc: 0.3535 - val_loss: 2.4229 - val_acc: 0.4850\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.6916 - acc: 0.3633 - val_loss: 2.3132 - val_acc: 0.5375\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.7083 - acc: 0.3379 - val_loss: 2.3829 - val_acc: 0.5513\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.6920 - acc: 0.3457 - val_loss: 2.3188 - val_acc: 0.5650\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 2.5278 - acc: 0.4043 - val_loss: 2.3525 - val_acc: 0.5183\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.5861 - acc: 0.3828 - val_loss: 2.2825 - val_acc: 0.5700\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.5488 - acc: 0.3711 - val_loss: 2.2527 - val_acc: 0.5337\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.5670 - acc: 0.3789 - val_loss: 2.2749 - val_acc: 0.5675\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.4862 - acc: 0.4023 - val_loss: 2.2289 - val_acc: 0.5513\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.4500 - acc: 0.3926 - val_loss: 2.1792 - val_acc: 0.5737\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 2.5343 - acc: 0.4082 - val_loss: 2.1930 - val_acc: 0.5841\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.5061 - acc: 0.3926 - val_loss: 2.1812 - val_acc: 0.5750\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.4539 - acc: 0.4180 - val_loss: 2.1976 - val_acc: 0.5950\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.4957 - acc: 0.4023 - val_loss: 2.0162 - val_acc: 0.6262\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.5257 - acc: 0.3730 - val_loss: 2.1235 - val_acc: 0.5650\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.4548 - acc: 0.4160 - val_loss: 2.1703 - val_acc: 0.5700\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 2.3864 - acc: 0.4414 - val_loss: 2.0383 - val_acc: 0.6043\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 91s 3s/step - loss: 2.3873 - acc: 0.4281 - val_loss: 2.0675 - val_acc: 0.5975\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.3261 - acc: 0.4395 - val_loss: 2.0078 - val_acc: 0.6288\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2862 - acc: 0.4453 - val_loss: 2.0503 - val_acc: 0.5913\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2363 - acc: 0.4434 - val_loss: 2.0167 - val_acc: 0.5975\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.3241 - acc: 0.4609 - val_loss: 2.0113 - val_acc: 0.6062\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2921 - acc: 0.4355 - val_loss: 2.0025 - val_acc: 0.6068\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2622 - acc: 0.4473 - val_loss: 1.9743 - val_acc: 0.6025\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.3441 - acc: 0.4258 - val_loss: 1.8803 - val_acc: 0.6388\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2097 - acc: 0.4844 - val_loss: 1.9749 - val_acc: 0.6038\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2038 - acc: 0.4688 - val_loss: 1.9159 - val_acc: 0.6325\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.2016 - acc: 0.4629 - val_loss: 1.8603 - val_acc: 0.6525\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.1769 - acc: 0.4746 - val_loss: 1.8969 - val_acc: 0.6713\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.1300 - acc: 0.4746 - val_loss: 1.8647 - val_acc: 0.6288\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.1790 - acc: 0.4668 - val_loss: 1.8497 - val_acc: 0.6663\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.1617 - acc: 0.4844 - val_loss: 1.8530 - val_acc: 0.6150\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.0874 - acc: 0.4922 - val_loss: 1.8580 - val_acc: 0.6587\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.0530 - acc: 0.5215 - val_loss: 1.8059 - val_acc: 0.6275\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.0972 - acc: 0.4766 - val_loss: 1.7307 - val_acc: 0.6600\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.1073 - acc: 0.4922 - val_loss: 1.7107 - val_acc: 0.6498\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.0546 - acc: 0.4902 - val_loss: 1.7714 - val_acc: 0.6613\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 92s 3s/step - loss: 2.0793 - acc: 0.4707 - val_loss: 1.7662 - val_acc: 0.6763\n",
      "Epoch 93/100\n",
      "28/32 [=========================>....] - ETA: 4s - loss: 2.0425 - acc: 0.4844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_class = 95\n",
    "conv_base = VGG19(weights = 'imagenet',\n",
    "                  include_top = False,\n",
    "                  input_shape=(image_width,image_height,3))\n",
    "#conv_base.summary()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class,activation='softmax'))\n",
    "\n",
    "print('The number of trainable weights before conv_base is frozen:', len(model.trainable_weights))\n",
    "# avoid the weights from pretrained model are modified during training \n",
    "conv_base.trainable = False\n",
    "print('The number of trainable weights after conv_base is frozen:', len(model.trainable_weights))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "fit_model_vgg = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=32,validation_steps=50,epochs=100,verbose=1,\n",
    "                    validation_data=validation_generator)\n",
    "performance = model.evaluate_generator(test_generator,steps=32)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable weights before conv_base is frozen: 216\n",
      "The number of trainable weights after conv_base is frozen: 4\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 56s 2s/step - loss: 6.5034 - acc: 0.0449 - val_loss: 4.9698 - val_acc: 0.0163\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 4.8907 - acc: 0.0938 - val_loss: 4.9005 - val_acc: 0.0225\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 4.3363 - acc: 0.1289 - val_loss: 4.9291 - val_acc: 0.0175\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 3.9086 - acc: 0.1875 - val_loss: 4.8551 - val_acc: 0.0187\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 3.5016 - acc: 0.2246 - val_loss: 4.9163 - val_acc: 0.0213\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 3.3794 - acc: 0.2598 - val_loss: 4.8887 - val_acc: 0.0125\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 3.1010 - acc: 0.3066 - val_loss: 4.8682 - val_acc: 0.0152\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 2.8656 - acc: 0.3379 - val_loss: 4.9772 - val_acc: 0.0088\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 2.5784 - acc: 0.3945 - val_loss: 4.9911 - val_acc: 0.0075\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 2.4341 - acc: 0.4258 - val_loss: 5.0109 - val_acc: 0.0200\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 49s 2s/step - loss: 2.3172 - acc: 0.4473 - val_loss: 5.0160 - val_acc: 0.0187\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 2.2532 - acc: 0.4609 - val_loss: 5.0722 - val_acc: 0.0200\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 2.1570 - acc: 0.4844 - val_loss: 5.0758 - val_acc: 0.0114\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 1.8458 - acc: 0.5352 - val_loss: 5.0550 - val_acc: 0.0187\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.8202 - acc: 0.5527 - val_loss: 5.0639 - val_acc: 0.0088\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.8423 - acc: 0.5469 - val_loss: 5.2369 - val_acc: 0.0112\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.5703 - acc: 0.5957 - val_loss: 5.1473 - val_acc: 0.0200\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 48s 2s/step - loss: 1.5466 - acc: 0.6191 - val_loss: 5.1725 - val_acc: 0.0175\n",
      "Epoch 19/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 1.4248 - acc: 0.6371"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0f52c2eccbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m fit_model=model_resnet.fit_generator(train_generator,\n\u001b[1;32m     23\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     validation_data=validation_generator)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_resnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The test loss is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_class = 95\n",
    "conv_base = ResNet50(weights = 'imagenet',\n",
    "                  include_top = False,\n",
    "                  input_shape=(image_width,image_height,3))\n",
    "#conv_base.summary()\n",
    "\n",
    "model_resnet = Sequential()\n",
    "model_resnet.add(conv_base)\n",
    "model_resnet.add(Flatten())\n",
    "model_resnet.add(Dense(1024,activation='relu'))\n",
    "model_resnet.add(Dropout(0.5))\n",
    "model_resnet.add(Dense(number_of_class,activation='softmax'))\n",
    "\n",
    "print('The number of trainable weights before conv_base is frozen:', len(model_resnet.trainable_weights))\n",
    "# avoid the weights from pretrained model are modified during training \n",
    "conv_base.trainable = False\n",
    "print('The number of trainable weights after conv_base is frozen:', len(model_resnet.trainable_weights))\n",
    "\n",
    "model_resnet.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "fit_model=model_resnet.fit_generator(train_generator,\n",
    "                    steps_per_epoch=32,validation_steps=50,epochs=100,verbose=1,\n",
    "                    validation_data=validation_generator)\n",
    "performance = model_resnet.evaluate_generator(test_generator,steps=32)\n",
    "print(\"The test loss is\",performance[0])\n",
    "print(\"The test accuracy is\",performance[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
